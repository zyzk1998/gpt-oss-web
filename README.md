✨ GPT-OSS 本地 Web 对话平台



📖 项目概述

本项目基于 Ollama 工具链本地化部署 gpt-oss:latest 大语言模型，通过轻量 Web 界面实现自然语言对话能力。核心优势在于 数据不经过第三方服务器，兼顾隐私安全与使用便捷性，支持团队、个人多场景共享访问。

无需依赖 OpenAI 等第三方 API，部署后即可通过浏览器随时随地与模型交互，非技术用户也能快速上手。

🖼️ 功能预览

简洁直观的对话界面，支持消息历史、加载状态提示、错误反馈，适配电脑/手机等设备：

✅ 区分用户/模型消息气泡
✅ 实时打字加载动画
✅ 支持 Shift+Enter 换行输入
✅ 响应式设计，手机端友好

🚀 核心功能

🔒 本地化部署

模型全程运行于本地服务器，对话数据闭环存储，避免隐私泄露风险，适用于敏感数据场景。

🌐 全网络支持

兼容内网（同路由器）与外网（公网）访问，配置完成后，异地设备也能通过链接调用服务。

👥 多人共享

无客户端 IP 限制，支持多用户同时对话，团队协作、小型办公场景可直接共享使用。

⚡ 优化体验

预设合理模型参数（温度 0.7/Top-P 0.9），平衡回复质量与速度，搭配加载动画、错误提示等交互细节。

🔧 稳定架构

通过 Nginx 反向代理解决跨域问题，合并 Web 与模型服务端口，降低部署复杂度，提升服务稳定性。

📱 多端适配

Web 界面支持响应式布局，电脑、手机、平板等设备访问时自动适配屏幕尺寸，使用体验一致。

⚙️ 快速启动

前提：服务器已部署 Ollama 及 Nginx 服务，且 gpt-oss:latest 模型已拉取。

1. 启动 Ollama 服务：
        export OLLAMA_HOST=0.0.0.0:8000
export OLLAMA_ORIGINS='*'
nohup ollama serve &

2. 配置 Nginx 反向代理：参考项目部署文档，确保 8082 端口正常监听。

3. 放置 Web 文件：将 gpt.html 放入 Nginx 配置的网页根目录。

4. 访问服务：通过下方地址打开对话界面，直接使用。

🌍 快速访问

访问场景

访问地址

说明

内网环境

ollama在11434端口运行

网页服务部署： nohup ollama serve --host 0.0.0.0:11434 --origins "http://192.168.8.111:8082" &

任意计算机打开 http://[服务器内网IP]:8082

同网络下设备直接访问

外网环境

http://[服务器公网IP]:8082

需提前配置路由器端口转发

使用方式：打开地址后，在输入框填写问题并点击「发送」，即可获取模型回复（Shift+Enter 可换行）。

⚠️ 注意事项

- 资源占用：模型运行需占用 CPU/GPU 及内存资源，建议根据硬件配置调整并发量，避免服务卡顿。

- 网络配置：外网访问需确保服务器防火墙开放 8082 端口，且路由器已配置端口转发规则。

- 安全防护：公开服务时建议限制访问范围（如通过 Nginx 配置 IP 白名单），避免恶意请求消耗资源。

- 日志排查：服务异常时可通过 Ollama 日志（nohup.out）定位问题，常见错误包括端口占用、模型缺失等。

📌 常见问题

Q1：网页提示“请求失败”怎么办？
A1：检查 Ollama 服务是否正常运行（ps aux | grep ollama），及 Nginx 配置是否正确（nginx -t）。

Q2：手机访问时界面错乱？
A2：清除浏览器缓存后重新访问，本项目已适配移动端，若仍异常可尝试使用 Chrome/Edge 浏览器。
