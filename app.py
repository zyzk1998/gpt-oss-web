import os
import io
import json
import base64
import traceback
import contextlib
import uvicorn
import shutil
from fastapi import FastAPI, Request, UploadFile, File
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
from typing import List, Optional, Union, Dict, Any
from dotenv import load_dotenv

# --- LangChain & BioBlend ---
from bioblend.galaxy import GalaxyInstance
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage

# 1. åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ================= é…ç½®åŒºåŸŸ =================
PORT = 8082
EMBED_MODEL = "nomic-embed-text"
LLM_MODEL = "gpt-oss:latest"
VISION_MODEL = "llama3.2-vision:11b"
OLLAMA_URL = "http://localhost:11434"
VECTOR_DB_PATH = "./data/chroma_db_bioblend"

GALAXY_URL = os.getenv("GALAXY_URL", "https://usegalaxy.org")
GALAXY_KEY = os.getenv("GALAXY_API_KEY", "")
# ===========================================

# --- 2. åˆå§‹åŒ– Galaxy è¿æ¥ ---
gi = None
print(f">>> [ç³»ç»Ÿ] æ­£åœ¨è¿æ¥ Galaxy ({GALAXY_URL})...")
try:
    gi = GalaxyInstance(url=GALAXY_URL, key=GALAXY_KEY)
    user = gi.users.get_current_user()
    print(f">>> [ç³»ç»Ÿ] Galaxy è¿æ¥æˆåŠŸ! å½“å‰ç”¨æˆ·: {user.get('username', 'Unknown')}")
except Exception as e:
    print(f">>> [ä¸¥é‡é”™è¯¯] Galaxy è¿æ¥å¤±è´¥: {e}")

# --- 3. å…¨èƒ½æ™ºèƒ½ä½“å®šä¹‰ ---
class BioBlendAgent:
    def __init__(self):
        self.vector_db = self._load_db()
        # å¤§è„‘: è´Ÿè´£æ¨ç†å’Œä»£ç ç”Ÿæˆ (temperature=0.1 ä¿è¯é€»è¾‘ç¨³å®š)
        self.brain = ChatOllama(model=LLM_MODEL, base_url=OLLAMA_URL, temperature=0.1)
        # çœ¼ç›: è´Ÿè´£çœ‹å›¾
        self.eye = ChatOllama(model=VISION_MODEL, base_url=OLLAMA_URL, temperature=0)
        
        # ç³»ç»ŸåŸºç¡€èƒ½åŠ› (Source A)
        self.system_capabilities = """
        [Source A: Galaxy System APIs]
        1. Get Current User Info: gi.users.get_current_user()
        2. List Histories: gi.histories.get_histories()
        3. Upload File: gi.tools.upload_file('path', history_id)
        """

    def _load_db(self):
        if os.path.exists(VECTOR_DB_PATH) and os.listdir(VECTOR_DB_PATH):
            print(f">>> [ç³»ç»Ÿ] åŠ è½½å‘é‡åº“: {VECTOR_DB_PATH}")
            embeddings = OllamaEmbeddings(model=EMBED_MODEL, base_url=OLLAMA_URL)
            return Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)
        print(">>> [è­¦å‘Š] å‘é‡åº“æœªæ‰¾åˆ°ï¼Œè¯·å…ˆè¿è¡Œ rebuild_db.py")
        return None

    def ocr_image(self, image_path):
        """åˆ©ç”¨è§†è§‰æ¨¡å‹è¯†åˆ«å›¾ç‰‡å†…å®¹ (å¼ºåˆ¶ä¸­æ–‡è¾“å‡º)"""
        print(f"   [è§†è§‰] æ­£åœ¨è°ƒç”¨ {VISION_MODEL} è¿›è¡Œ OCR...")
        try:
            with open(image_path, "rb") as image_file:
                image_data = base64.b64encode(image_file.read()).decode("utf-8")
            
            # ã€ä¼˜åŒ–ã€‘Prompt: è¦æ±‚æå–æ–‡æœ¬å¹¶ç”¨ä¸­æ–‡æè¿°
            prompt_text = """
            Please analyze this image.
            1. Extract all visible text strictly.
            2. Briefly describe what this image is about in **Chinese** (Simplified).
            
            Output format:
            [è¯†åˆ«åˆ°çš„æ–‡å­—]: ...
            [å›¾ç‰‡æè¿°]: ...
            """
            
            message = HumanMessage(
                content=[
                    {"type": "text", "text": prompt_text},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
                ]
            )
            res = self.eye.invoke([message])
            return res.content
        except Exception as e:
            print(f"   [è§†è§‰é”™è¯¯] {e}")
            return f"OCR è¯†åˆ«å¤±è´¥: {str(e)}"

    def smart_process(self, user_query, file_context, chat_history=[], selected_tool=None):
        """
        æ ¸å¿ƒé€»è¾‘ï¼šRAG + è®°å¿† + å†³ç­–
        """
        if not self.vector_db:
            return {"type": "text", "reply": "âŒ é”™è¯¯ï¼šçŸ¥è¯†åº“æœªåŠ è½½ï¼Œæ— æ³•å·¥ä½œã€‚"}

        # A. ç”¨æˆ·å·²é€‰å®šå·¥å…· -> å¼ºåˆ¶ç”Ÿæˆä»£ç 
        if selected_tool:
            return self._generate_code_only(user_query, file_context, selected_tool)

        # B. å¸¸è§„æµç¨‹
        print(f"   [æ€è€ƒ] ç”¨æˆ·éœ€æ±‚: {user_query}")
        
        # 1. æ£€ç´¢ (Source B)
        retriever = self.vector_db.as_retriever(search_kwargs={"k": 5})
        docs = retriever.invoke(user_query)
        retrieved_tools = "\n".join([f"- Tool {i+1}: {d.page_content}" for i, d in enumerate(docs)])

        # 2. æ ¼å¼åŒ–å†å²è®°å½• (åªå–æœ€è¿‘ 3 è½®ï¼Œé¿å… Token æº¢å‡º)
        history_text = ""
        if chat_history:
            history_text = "\n".join([f"User: {h.get('user','')}\nAI: {h.get('ai','')}" for h in chat_history[-3:]])

        # 3. æ„é€  Prompt (å¢åŠ è¯­è¨€çº¦æŸå’Œè®°å¿†æ§½)
        template = """
        You are a Galaxy BioBlend Expert. 
        
        ã€Language Rulesã€‘
        1. **Follow User's Language**: If the user asks in Chinese, you MUST reply in Chinese. If English, reply in English.
        2. **Exception**: Do NOT translate the Python code or Galaxy Tool Names.
        
        ã€Conversation Historyã€‘
        {history}
        
        ã€Current Requestã€‘
        User: "{query}"
        File Status: {file_context}
        
        ã€Knowledge Baseã€‘
        {system_caps}
        
        [Source B: Retrieved Tools]
        {retrieved_tools}
        
        ã€Decision Logicã€‘
        1. **System API**: If it matches Source A (e.g., "who am I"), generate code.
        2. **Tool Run**: If it matches Source B (e.g., "run FastQC"), generate code.
        3. **Missing File**: If tool needs file but File Status is empty, reply in Chinese: "è¯·å…ˆä¸Šä¼ æ–‡ä»¶ (Please upload file first)."
        4. **Ambiguous**: If unsure, return a JSON list.
        5. **Chat**: If the user is just chatting or asking about previous results (based on History), answer them naturally.
        
        ã€Output Formatã€‘
        - Code: ```python ... ```
        - List: ```json ... ```
        - Text: Plain text (in User's Language).
        """
        
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.brain | StrOutputParser()
        
        print("   [æ¨ç†] gpt-oss æ­£åœ¨å†³ç­–...")
        response = chain.invoke({
            "query": user_query,
            "file_context": str(file_context),
            "system_caps": self.system_capabilities,
            "retrieved_tools": retrieved_tools,
            "history": history_text
        })
        
        return self._parse_llm_response(response)

    def _generate_code_only(self, query, file_context, tool_info):
        """å¼ºåˆ¶ç”Ÿæˆä»£ç  (ç”¨äºç”¨æˆ·é€‰å®šå·¥å…·å)"""
        template = """
        You are a Galaxy BioBlend Expert. User selected tool "{tool_name}" (ID: {tool_id}).
        User Request: "{query}"
        File Status: {file_context}
        
        Task: Write Python code using `gi.tools.run_tool`.
        Requirements:
        1. Assume `gi` is connected.
        2. Use file ID from status if available.
        3. MUST print result.
        4. Return ONLY Python code inside ```python```.
        """
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.brain | StrOutputParser()
        response = chain.invoke({
            "tool_name": tool_info['name'],
            "tool_id": tool_info['id'],
            "query": query,
            "file_context": str(file_context)
        })
        return self._parse_llm_response(response)

    def _parse_llm_response(self, response):
        """è§£æ LLM è¿”å›çš„æ··åˆæ ¼å¼"""
        response = response.strip()
        
        # 1. è¯†åˆ«ä»£ç å— -> æ‰§è¡Œ
        if "```python" in response:
            code = response.split("```python")[1].split("```")[0].strip()
            exec_out = self._execute_code_sandbox(code)
            final_reply = f"### ğŸ¤– ç­–ç•¥ä»£ç \n```python\n{code}\n```\n### âœ… æ‰§è¡Œç»“æœ\n```text\n{exec_out}\n```"
            return {"type": "text", "reply": final_reply}
        
        # 2. è¯†åˆ« JSON (å·¥å…·åˆ—è¡¨) -> å‰ç«¯é€‰æ‹©
        elif "```json" in response:
            try:
                json_str = response.split("```json")[1].split("```")[0].strip()
                candidates = json.loads(json_str)
                return {
                    "type": "choice",
                    "reply": "æ‰¾åˆ°å¤šä¸ªç›¸å…³å·¥å…·ï¼Œè¯·é€‰æ‹©ï¼š",
                    "candidates": candidates
                }
            except:
                return {"type": "text", "reply": response}
        
        # 3. å°è¯•ç›´æ¥è§£æ JSON
        elif response.startswith("[") and response.endswith("]"):
            try:
                candidates = json.loads(response)
                return {"type": "choice", "reply": "è¯·é€‰æ‹©å·¥å…·ï¼š", "candidates": candidates}
            except:
                pass

        # 4. é»˜è®¤æ–‡æœ¬
        return {"type": "text", "reply": response}

    def _execute_code_sandbox(self, code):
        """æ²™ç®±æ‰§è¡Œä»£ç """
        if not gi: return "Galaxy æœªè¿æ¥ï¼Œæ— æ³•æ‰§è¡Œã€‚"
        
        output_buffer = io.StringIO()
        try:
            # æ³¨å…¥å¿…è¦çš„å…¨å±€å˜é‡
            sandbox = {"gi": gi, "json": json, "print": print}
            with contextlib.redirect_stdout(output_buffer):
                exec(code, sandbox)
            result = output_buffer.getvalue()
            return result if result else "(ä»£ç æ‰§è¡ŒæˆåŠŸï¼Œä½†æ²¡æœ‰ print è¾“å‡º)"
        except Exception:
            return traceback.format_exc()

# --- 4. Web åº”ç”¨åˆå§‹åŒ– ---
app = FastAPI()
templates = Jinja2Templates(directory="templates")
agent = BioBlendAgent()

# ã€ä¼˜åŒ–ã€‘è¯·æ±‚æ¨¡å‹å¢åŠ  history å­—æ®µ
class ChatRequest(BaseModel):
    message: str
    history: List[Dict[str, str]] = [] 
    selected_tool: Optional[Dict[str, Any]] = None
    uploaded_file_id: Optional[str] = None
    uploaded_file_name: Optional[str] = None

# --- è·¯ç”±å®šä¹‰ ---

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/api/upload")
async def upload_handler(file: UploadFile = File(...)):
    """å¤„ç†æ–‡ä»¶ä¸Šä¼ ï¼šå›¾ç‰‡->OCRï¼Œæ•°æ®->Galaxy"""
    try:
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        temp_path = f"temp_{file.filename}"
        with open(temp_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
            
        result = {"status": "success", "file_name": file.filename}
        
        # åˆ†æ”¯ A: å›¾ç‰‡ (OCR)
        if file.filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.webp')):
            print(f">>> [ä¸Šä¼ ] æ£€æµ‹åˆ°å›¾ç‰‡ï¼Œå¯åŠ¨ Vision æ¨¡å‹...")
            ocr_text = agent.ocr_image(temp_path)
            result["type"] = "image"
            result["ocr_text"] = ocr_text
            os.remove(temp_path) 
            
        # åˆ†æ”¯ B: æ•°æ®æ–‡ä»¶ (ä¸Šä¼  Galaxy)
        else:
            if gi:
                print(f">>> [ä¸Šä¼ ] ä¸Šä¼ æ•°æ®åˆ° Galaxy...")
                histories = gi.histories.get_histories()
                hid = histories[0]['id'] if histories else gi.histories.create_history("GPT-OSS Analysis")['id']
                
                ret = gi.tools.upload_file(temp_path, hid)
                result["type"] = "data"
                result["file_id"] = ret['outputs'][0]['id']
                os.remove(temp_path)
            else:
                return {"status": "error", "message": "Galaxy æœªè¿æ¥"}
                
        return result

    except Exception as e:
        traceback.print_exc()
        return {"status": "error", "message": str(e)}

@app.post("/api/chat")
async def chat_handler(req: ChatRequest):
    # æ„å»ºä¸Šä¸‹æ–‡
    file_ctx = {
        "has_file": bool(req.uploaded_file_id),
        "file_id": req.uploaded_file_id,
        "file_name": req.uploaded_file_name
    }
    
    # ç»Ÿä¸€å…¥å£ï¼Œä¼ å…¥ history
    response = agent.smart_process(req.message, file_ctx, req.history, req.selected_tool)
    return response

if __name__ == "__main__":
    print(f">>> [å¯åŠ¨] æœåŠ¡è¿è¡Œåœ¨: http://0.0.0.0:{PORT}")
    uvicorn.run(app, host="0.0.0.0", port=PORT)
