import os
import io
import json
import base64
import traceback
import contextlib
import uvicorn
import shutil
from fastapi import FastAPI, Request, UploadFile, File
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
from typing import List, Optional, Union, Dict, Any
from dotenv import load_dotenv

# --- LangChain & BioBlend ---
from bioblend.galaxy import GalaxyInstance
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage

load_dotenv()

# ================= é…ç½®åŒºåŸŸ =================
PORT = 8082
EMBED_MODEL = "nomic-embed-text"
LLM_MODEL = "gpt-oss:latest"
VISION_MODEL = "llama3.2-vision:11b"
OLLAMA_URL = "http://localhost:11434"
VECTOR_DB_PATH = "./data/chroma_db_bioblend"
GALAXY_URL = os.getenv("GALAXY_URL", "https://usegalaxy.org")
GALAXY_KEY = os.getenv("GALAXY_API_KEY", "")
# ===========================================

gi = None
print(f">>> [ç³»ç»Ÿ] æ­£åœ¨è¿æ¥ Galaxy ({GALAXY_URL})...")
try:
    gi = GalaxyInstance(url=GALAXY_URL, key=GALAXY_KEY)
    user = gi.users.get_current_user()
    print(f">>> [ç³»ç»Ÿ] Galaxy è¿æ¥æˆåŠŸ! å½“å‰ç”¨æˆ·: {user.get('username', 'Unknown')}")
except Exception as e:
    print(f">>> [ä¸¥é‡é”™è¯¯] Galaxy è¿æ¥å¤±è´¥: {e}")

class BioBlendAgent:
    def __init__(self):
        self.vector_db = self._load_db()
        self.brain = ChatOllama(model=LLM_MODEL, base_url=OLLAMA_URL, temperature=0.1)
        self.eye = ChatOllama(model=VISION_MODEL, base_url=OLLAMA_URL, temperature=0)
        
        self.system_capabilities = """
        [Source A: Galaxy System APIs]
        1. Get Current User Info: gi.users.get_current_user()
        2. List Histories: gi.histories.get_histories()
        3. Upload File: gi.tools.upload_file('path', history_id)
        """

    def _load_db(self):
        if os.path.exists(VECTOR_DB_PATH) and os.listdir(VECTOR_DB_PATH):
            embeddings = OllamaEmbeddings(model=EMBED_MODEL, base_url=OLLAMA_URL)
            return Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)
        return None

    def ocr_image(self, image_path):
        """OCR è¯†åˆ«"""
        try:
            with open(image_path, "rb") as image_file:
                image_data = base64.b64encode(image_file.read()).decode("utf-8")
            
            prompt_text = """
            Please analyze this image.
            1. Extract all visible text strictly.
            2. Briefly describe what this image is about in **Chinese** (Simplified).
            Output format: [è¯†åˆ«åˆ°çš„æ–‡å­—]: ... [å›¾ç‰‡æè¿°]: ...
            """
            message = HumanMessage(content=[
                {"type": "text", "text": prompt_text},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
            ])
            res = self.eye.invoke([message])
            return res.content
        except Exception as e:
            return f"OCR è¯†åˆ«å¤±è´¥: {str(e)}"

    def smart_process(self, user_query, file_context, chat_history=[], selected_tool=None):
        if not self.vector_db:
            return {"type": "text", "reply": "âŒ é”™è¯¯ï¼šçŸ¥è¯†åº“æœªåŠ è½½ã€‚", "suggestions": []}

        if selected_tool:
            return self._generate_code_only(user_query, file_context, selected_tool)

        # RAG æµç¨‹
        retriever = self.vector_db.as_retriever(search_kwargs={"k": 5})
        docs = retriever.invoke(user_query)
        retrieved_tools = "\n".join([f"- Tool {i+1}: {d.page_content}" for i, d in enumerate(docs)])

        history_text = ""
        if chat_history:
            history_text = "\n".join([f"User: {h.get('user','')}\nAI: {h.get('ai','')}" for h in chat_history[-3:]])

        template = """
        You are a Galaxy BioBlend Expert. 
        ã€Language Rulesã€‘Follow User's Language (Chinese -> Chinese).
        ã€Historyã€‘{history}
        ã€Requestã€‘User: "{query}" | File: {file_context}
        ã€Knowledgeã€‘{system_caps}
        [Source B] {retrieved_tools}
        
        ã€Decision Logicã€‘
        1. System API -> Code.
        2. Tool Run -> Code.
        3. Missing File -> Reply "Please upload file first" in Chinese.
        4. Ambiguous -> JSON List.
        5. Chat -> Natural answer.
        
        ã€Outputã€‘Code: ```python ... ``` | List: ```json ... ``` | Text: Plain text.
        """
        
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.brain | StrOutputParser()
        
        response = chain.invoke({
            "query": user_query,
            "file_context": str(file_context),
            "system_caps": self.system_capabilities,
            "retrieved_tools": retrieved_tools,
            "history": history_text
        })
        
        return self._parse_llm_response(response, context="chat")

    def _generate_code_only(self, query, file_context, tool_info):
        template = """
        You are a Galaxy BioBlend Expert. User selected tool "{tool_name}" (ID: {tool_id}).
        User Request: "{query}" | File: {file_context}
        Task: Write Python code using `gi.tools.run_tool`. Print result.
        Return ONLY Python code inside ```python```.
        """
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.brain | StrOutputParser()
        response = chain.invoke({
            "tool_name": tool_info['name'],
            "tool_id": tool_info['id'],
            "query": query,
            "file_context": str(file_context)
        })
        return self._parse_llm_response(response, context="execution")

    def _parse_llm_response(self, response, context="chat"):
        """è§£æå“åº”å¹¶ç”Ÿæˆå¼•å¯¼å»ºè®®"""
        response = response.strip()
        suggestions = []

        # 1. ä»£ç æ‰§è¡Œç»“æœ
        if "```python" in response:
            code = response.split("```python")[1].split("```")[0].strip()
            exec_out = self._execute_code_sandbox(code)
            final_reply = f"### ğŸ¤– ç­–ç•¥ä»£ç \n```python\n{code}\n```\n### âœ… æ‰§è¡Œç»“æœ\n```text\n{exec_out}\n```"
            
            # ã€æ–°å¢ã€‘ç”Ÿæˆå¼•å¯¼å»ºè®®
            suggestions = [
                "èƒ½è§£é‡Šä¸€ä¸‹è¿™ä¸ªç»“æœå—ï¼Ÿ",
                "å¦‚ä½•å°†è¿™äº›æ•°æ®å¯è§†åŒ–ï¼Ÿ",
                "å¸®æˆ‘æŠŠç»“æœä¿å­˜åˆ°æœ¬åœ°"
            ]
            return {"type": "text", "reply": final_reply, "suggestions": suggestions}
        
        # 2. å·¥å…·é€‰æ‹©åˆ—è¡¨
        elif "```json" in response:
            try:
                candidates = json.loads(response.split("```json")[1].split("```")[0].strip())
                return {"type": "choice", "reply": "æ‰¾åˆ°å¤šä¸ªç›¸å…³å·¥å…·ï¼Œè¯·é€‰æ‹©ï¼š", "candidates": candidates, "suggestions": []}
            except:
                return {"type": "text", "reply": response, "suggestions": []}
        
        # 3. çº¯æ–‡æœ¬ (å¯èƒ½æ˜¯ç¼ºæ–‡ä»¶æç¤ºï¼Œæˆ–è€…é—²èŠ)
        else:
            # ç®€å•çš„å…³é”®è¯åŒ¹é…æ¥ç”Ÿæˆå»ºè®®
            if "ä¸Šä¼ " in response or "upload" in response.lower():
                suggestions = ["å¦‚ä½•è·å–ç¤ºä¾‹æ•°æ®ï¼Ÿ", "æ”¯æŒå“ªäº›æ–‡ä»¶æ ¼å¼ï¼Ÿ"]
            elif "å†å²" in response:
                suggestions = ["åˆ—å‡ºæœ€è¿‘çš„ dataset", "æ¸…ç†å†å²è®°å½•"]
            
            return {"type": "text", "reply": response, "suggestions": suggestions}

    def _execute_code_sandbox(self, code):
        if not gi: return "Galaxy æœªè¿æ¥"
        output_buffer = io.StringIO()
        try:
            sandbox = {"gi": gi, "json": json, "print": print}
            with contextlib.redirect_stdout(output_buffer):
                exec(code, sandbox)
            return output_buffer.getvalue() or "(æ— è¾“å‡º)"
        except Exception:
            return traceback.format_exc()

app = FastAPI()
templates = Jinja2Templates(directory="templates")
agent = BioBlendAgent()

class ChatRequest(BaseModel):
    message: str
    history: List[Dict[str, str]] = [] 
    selected_tool: Optional[Dict[str, Any]] = None
    uploaded_file_id: Optional[str] = None
    uploaded_file_name: Optional[str] = None

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/api/upload")
async def upload_handler(file: UploadFile = File(...)):
    try:
        temp_path = f"temp_{file.filename}"
        with open(temp_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
            
        result = {"status": "success", "file_name": file.filename}
        
        if file.filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.webp')):
            ocr_text = agent.ocr_image(temp_path)
            result["type"] = "image"
            result["ocr_text"] = ocr_text
            # å›¾ç‰‡è¯†åˆ«åçš„å»ºè®®
            result["suggestions"] = ["æå–å…¶ä¸­çš„åºåˆ—ä¿¡æ¯", "è§£é‡ŠæŠ¥é”™åŸå› ", "ç¿»è¯‘æˆä¸­æ–‡"]
            os.remove(temp_path) 
        else:
            if gi:
                histories = gi.histories.get_histories()
                hid = histories[0]['id'] if histories else gi.histories.create_history("GPT-OSS Analysis")['id']
                ret = gi.tools.upload_file(temp_path, hid)
                result["type"] = "data"
                result["file_id"] = ret['outputs'][0]['id']
                # æ•°æ®ä¸Šä¼ åçš„å»ºè®®
                result["suggestions"] = ["å¯¹è¿™ä¸ªæ–‡ä»¶åšè´¨æ§", "æŸ¥çœ‹æ–‡ä»¶å‰10è¡Œ", "æ¯”å¯¹åˆ°å‚è€ƒåŸºå› ç»„"]
                os.remove(temp_path)
            else:
                return {"status": "error", "message": "Galaxy æœªè¿æ¥"}
        return result
    except Exception as e:
        traceback.print_exc()
        return {"status": "error", "message": str(e)}

@app.post("/api/chat")
async def chat_handler(req: ChatRequest):
    file_ctx = {
        "has_file": bool(req.uploaded_file_id),
        "file_id": req.uploaded_file_id,
        "file_name": req.uploaded_file_name
    }
    response = agent.smart_process(req.message, file_ctx, req.history, req.selected_tool)
    return response

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=PORT)
