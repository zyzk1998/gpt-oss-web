import os
import io
import json
import base64
import traceback
import contextlib
import uvicorn
import shutil
from fastapi import FastAPI, Request, UploadFile, File
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
from typing import List, Optional, Union, Dict, Any
from dotenv import load_dotenv

# --- LangChain & BioBlend ---
from bioblend.galaxy import GalaxyInstance
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage

load_dotenv()

# ================= é…ç½®åŒºåŸŸ =================
PORT = 8082
EMBED_MODEL = "nomic-embed-text"
LLM_MODEL = "gpt-oss:latest"
VISION_MODEL = "llama3.2-vision:11b"
OLLAMA_URL = "http://localhost:11434"
VECTOR_DB_PATH = "./data/chroma_db_bioblend"
GALAXY_URL = os.getenv("GALAXY_URL", "https://usegalaxy.org")
GALAXY_KEY = os.getenv("GALAXY_API_KEY", "")
# ===========================================

gi = None
print(f">>> [ç³»ç»Ÿ] æ­£åœ¨è¿æ¥ Galaxy ({GALAXY_URL})...")
try:
    gi = GalaxyInstance(url=GALAXY_URL, key=GALAXY_KEY)
    user = gi.users.get_current_user()
    print(f">>> [ç³»ç»Ÿ] Galaxy è¿æ¥æˆåŠŸ! å½“å‰ç”¨æˆ·: {user.get('username', 'Unknown')}")
except Exception as e:
    print(f">>> [ä¸¥é‡é”™è¯¯] Galaxy è¿æ¥å¤±è´¥: {e}")

class BioBlendAgent:
    def __init__(self):
        self.vector_db = self._load_db()
        self.brain = ChatOllama(model=LLM_MODEL, base_url=OLLAMA_URL, temperature=0.1)
        self.eye = ChatOllama(model=VISION_MODEL, base_url=OLLAMA_URL, temperature=0)
        
        # Source A: ç¡¬ç¼–ç çš„ç³»ç»Ÿå¸¸è¯† (å†…åŠ¡åºœ)
        self.system_capabilities = """
        [Source A: System Kernel (Management)]
        1. Get Current User Info: gi.users.get_current_user()
        2. List Histories: gi.histories.get_histories()
        3. Upload File: gi.tools.upload_file('path', history_id)
        """

    def _load_db(self):
        if os.path.exists(VECTOR_DB_PATH) and os.listdir(VECTOR_DB_PATH):
            embeddings = OllamaEmbeddings(model=EMBED_MODEL, base_url=OLLAMA_URL)
            return Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)
        return None

    def ocr_image(self, image_path):
        """OCR è¯†åˆ«"""
        try:
            with open(image_path, "rb") as image_file:
                image_data = base64.b64encode(image_file.read()).decode("utf-8")
            
            prompt_text = """
            Please analyze this image.
            1. Extract all visible text strictly.
            2. Briefly describe what this image is about in **Chinese** (Simplified).
            Output format: [è¯†åˆ«åˆ°çš„æ–‡å­—]: ... [å›¾ç‰‡æè¿°]: ...
            """
            message = HumanMessage(content=[
                {"type": "text", "text": prompt_text},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
            ])
            res = self.eye.invoke([message])
            return res.content
        except Exception as e:
            return f"OCR è¯†åˆ«å¤±è´¥: {str(e)}"

    def _analyze_domain(self, query):
        """
        ã€ç¬¬ä¸€æ­¥ï¼šé¢†åŸŸæ„ŸçŸ¥æ€»é—¸ã€‘
        åˆ¤æ–­ç”¨æˆ·æ„å›¾æ˜¯â€œç”Ÿä¿¡/Galaxyâ€è¿˜æ˜¯â€œé€šç”¨/é—²èŠâ€ã€‚
        """
        template = """
        You are a classifier. Analyze the user's input and determine the **Domain**.
        User Input: "{query}"
        
        Rules for [GALAXY_BIO]:
        - Keywords: Galaxy, BioBlend, tools, fastq, bam, genome, sequencing, workflow, analysis, QC.
        - Python code using `bioblend` or `galaxy`.
        - Questions about bioinformatics tasks.
        
        Rules for [GENERAL]:
        - General Python errors (e.g., SyntaxError, NameError, AttributeError) WITHOUT Galaxy context.
        - General coding questions (e.g., "how to write a loop", "explain this code").
        - Chit-chat (e.g., "hello", "who are you", "write a poem").
        
        Output ONLY one word: "GALAXY_BIO" or "GENERAL".
        """
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.brain | StrOutputParser()
        try:
            domain = chain.invoke({"query": query[:1000]}).strip()
            print(f"   [æ„ŸçŸ¥] é¢†åŸŸåˆ¤å®š: {domain}")
            return domain
        except:
            return "GALAXY_BIO" # é»˜è®¤å…œåº•

    def smart_process(self, user_query, file_context, chat_history=[], selected_tool=None):
        """æ ¸å¿ƒé€»è¾‘ï¼šæ„ŸçŸ¥ -> è·¯ç”± -> å†³ç­– -> è¡ŒåŠ¨"""
        if not self.vector_db:
            return {"type": "text", "reply": "âŒ é”™è¯¯ï¼šçŸ¥è¯†åº“æœªåŠ è½½ã€‚", "suggestions": []}

        # --- 0. å¼ºåˆ¶æ‰§è¡Œé€šé“ (ç‚¹å‡»å¡ç‰‡) ---
        # ç”¨æˆ·æ˜ç¡®ç‚¹å‡»äº†å·¥å…·ï¼Œç›´æ¥è¿›å…¥æ‰§è¡Œé€»è¾‘
        if selected_tool:
            if not file_context['has_file']:
                 return {
                     "type": "text", 
                     "reply": f"æ‚¨é€‰æ‹©äº†è¿è¡Œ **{selected_tool['name']}**ï¼Œä½†è¿™éœ€è¦è¾“å…¥æ•°æ®ã€‚\n\nè¯·ç‚¹å‡»ä¸‹æ–¹çš„ ğŸ“‚ æŒ‰é’®ä¸Šä¼ æ–‡ä»¶ã€‚", 
                     "suggestions": ["å¦‚ä½•è·å–ç¤ºä¾‹æ•°æ®ï¼Ÿ"]
                 }
            return self._generate_code_only(user_query, file_context, selected_tool)

        # --- 1. é¢†åŸŸæ„ŸçŸ¥ (Domain Perception) ---
        domain = self._analyze_domain(user_query)

        # --- åˆ†æ”¯ A: é€šç”¨/é—²èŠæ¨¡å¼ (General Mode) ---
        if "GENERAL" in domain:
            print("   [è·¯ç”±] è¯†åˆ«ä¸ºé€šç”¨æ„å›¾ -> æ—è·¯å¤„ç† (Skip RAG)")
            template = """
            You are a helpful AI Assistant.
            User Input: "{query}"
            History: {history}
            
            Task: Answer the user's question directly using your general knowledge.
            - If it's a general Python error, explain the fix.
            - If it's chat, reply naturally.
            - **Language**: Follow user's language (Chinese -> Chinese).
            """
            history_text = "\n".join([f"User: {h.get('user','')}\nAI: {h.get('ai','')}" for h in chat_history[-3:]])
            prompt = ChatPromptTemplate.from_template(template)
            chain = prompt | self.brain | StrOutputParser()
            response = chain.invoke({"query": user_query, "history": history_text})
            
            # é€šç”¨æ¨¡å¼ä¸‹ä¸ç”Ÿæˆå¤æ‚çš„ç”Ÿä¿¡å»ºè®®
            return {"type": "text", "reply": response, "suggestions": []}

        # --- åˆ†æ”¯ B: ç”Ÿä¿¡/Galaxy ä¸“å®¶æ¨¡å¼ (Expert Mode) ---
        else:
            print("   [è·¯ç”±] è¯†åˆ«ä¸º Galaxy æ„å›¾ -> å¯åŠ¨ RAG è·¯ç”±å¼•æ“")
            
            # 2. æ£€ç´¢ (Source B: å…µå™¨åº“)
            retriever = self.vector_db.as_retriever(search_kwargs={"k": 5})
            docs = retriever.invoke(user_query)
            retrieved_tools = "\n".join([f"- {d.page_content}" for i, d in enumerate(docs)])

            # 3. å†å²ä¸Šä¸‹æ–‡
            history_text = "\n".join([f"User: {h.get('user','')}\nAI: {h.get('ai','')}" for h in chat_history[-3:]])

            # 4. ä¸“å®¶å†³ç­– Prompt (Source A vs Source B)
            template = """
            You are the intelligent router for the Galaxy Bioinformatics System.
            Your task is to map the User's Intent to the correct Knowledge Source (A or B) and execute the action.
            
            ã€Current Contextã€‘
            - User Input: "{query}"
            - File Status: {file_context}
            - History: {history}
            
            ã€Available Knowledge Sourcesã€‘
            
            ğŸ”· **SOURCE A: System Kernel (Management)**
            *Use this ONLY for account info, history lists, or connection checks.*
            {system_caps}
            
            ğŸ”¶ **SOURCE B: Tool Library (Analysis)**
            *Use this for ANY data processing, quality control, assembly, or tool recommendations.*
            {retrieved_tools}
            
            ã€Routing & Decision Logicã€‘
            
            1. **Analyze Intent**: 
               - Is the user asking about *System Status* (Who am I? What history?) -> **Route to Source A**.
               - Is the user asking about *Bio-Analysis* (How to QC? Run SPAdes?) -> **Route to Source B**.
            
            2. **Determine Action**:
               - **Consultation (å’¨è¯¢)**: User asks "how to" or "recommend". (Route: Source B) -> Return **JSON List**.
               - **Execution (æ‰§è¡Œ)**: User says "run/execute". (Route: Source B) -> Check File -> Return **Python Code**.
               - **Management (ç®¡ç†)**: User asks info. (Route: Source A) -> Return **Python Code**.
            
            3. **Safety Check**:
               - If routing to Execution but file is missing -> Reply "Please upload file" (in Chinese).
            
            ã€Output Formatã€‘
            - Code: ```python ... ```
            - List: ```json ... ```
            - Text: Plain text (Chinese).
            """
            
            prompt = ChatPromptTemplate.from_template(template)
            chain = prompt | self.brain | StrOutputParser()
            
            print("   [æ¨ç†] gpt-oss æ­£åœ¨è¿›è¡Œæ„å›¾è·¯ç”±ä¸å†³ç­–...")
            response = chain.invoke({
                "query": user_query,
                "file_context": str(file_context),
                "system_caps": self.system_capabilities,
                "retrieved_tools": retrieved_tools,
                "history": history_text
            })
            
            return self._parse_llm_response(response, context="chat")

    def _generate_code_only(self, query, file_context, tool_info):
        template = """
        You are a Galaxy BioBlend Expert. User selected tool "{tool_name}" (ID: {tool_id}).
        User Request: "{query}" | File: {file_context}
        Task: Write Python code using `gi.tools.run_tool`. Print result.
        Return ONLY Python code inside ```python```.
        """
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.brain | StrOutputParser()
        response = chain.invoke({
            "tool_name": tool_info['name'],
            "tool_id": tool_info['id'],
            "query": query,
            "file_context": str(file_context)
        })
        return self._parse_llm_response(response, context="execution")

    def _parse_llm_response(self, response, context="chat"):
        response = response.strip()
        suggestions = []
        
        # 1. ä»£ç æ‰§è¡Œ
        if "```python" in response:
            code = response.split("```python")[1].split("```")[0].strip()
            exec_out = self._execute_code_sandbox(code)
            final_reply = f"### ğŸ¤– ç­–ç•¥ä»£ç \n```python\n{code}\n```\n### âœ… æ‰§è¡Œç»“æœ\n```text\n{exec_out}\n```"
            suggestions = ["èƒ½è§£é‡Šä¸€ä¸‹è¿™ä¸ªç»“æœå—ï¼Ÿ", "å¦‚ä½•å°†è¿™äº›æ•°æ®å¯è§†åŒ–ï¼Ÿ", "ä¿å­˜ç»“æœåˆ°æœ¬åœ°"]
            return {"type": "text", "reply": final_reply, "suggestions": suggestions}
        
        # 2. å·¥å…·åˆ—è¡¨ (å’¨è¯¢æ¨¡å¼)
        elif "```json" in response:
            try:
                candidates = json.loads(response.split("```json")[1].split("```")[0].strip())
                return {
                    "type": "choice", 
                    "reply": "æ ¹æ®æ‚¨çš„éœ€æ±‚ï¼Œæˆ‘ä¸ºæ‚¨æ‰¾åˆ°äº†ä»¥ä¸‹å·¥å…·ã€‚è¯·é€‰æ‹©ä¸€ä¸ªå¼€å§‹ï¼š", 
                    "candidates": candidates, 
                    "suggestions": ["è¿™äº›å·¥å…·çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ", "æˆ‘è¯¥å‡†å¤‡ä»€ä¹ˆæ ¼å¼çš„æ•°æ®ï¼Ÿ"]
                }
            except:
                return {"type": "text", "reply": response, "suggestions": []}
        
        # 3. çº¯æ–‡æœ¬
        else:
            if "ä¸Šä¼ " in response or "upload" in response.lower():
                suggestions = ["å¦‚ä½•è·å–ç¤ºä¾‹æ•°æ®ï¼Ÿ", "æ”¯æŒå“ªäº›æ–‡ä»¶æ ¼å¼ï¼Ÿ"]
            elif "å†å²" in response:
                suggestions = ["åˆ—å‡ºæœ€è¿‘çš„ dataset", "æ¸…ç†å†å²è®°å½•"]
            return {"type": "text", "reply": response, "suggestions": suggestions}

    def _execute_code_sandbox(self, code):
        if not gi: return "Galaxy æœªè¿æ¥"
        output_buffer = io.StringIO()
        try:
            sandbox = {"gi": gi, "json": json, "print": print}
            with contextlib.redirect_stdout(output_buffer):
                exec(code, sandbox)
            return output_buffer.getvalue() or "(æ— è¾“å‡º)"
        except Exception:
            return traceback.format_exc()

app = FastAPI()
templates = Jinja2Templates(directory="templates")
agent = BioBlendAgent()

class ChatRequest(BaseModel):
    message: str
    history: List[Dict[str, str]] = [] 
    selected_tool: Optional[Dict[str, Any]] = None
    uploaded_file_id: Optional[str] = None
    uploaded_file_name: Optional[str] = None

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/api/upload")
async def upload_handler(file: UploadFile = File(...)):
    try:
        temp_path = f"temp_{file.filename}"
        with open(temp_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        result = {"status": "success", "file_name": file.filename}
        
        # å›¾ç‰‡ -> OCR
        if file.filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.webp')):
            ocr_text = agent.ocr_image(temp_path)
            result["type"] = "image"
            result["ocr_text"] = ocr_text
            result["suggestions"] = ["æå–å…¶ä¸­çš„åºåˆ—ä¿¡æ¯", "è§£é‡ŠæŠ¥é”™åŸå› ", "ç¿»è¯‘æˆä¸­æ–‡"]
            os.remove(temp_path) 
        # æ•°æ® -> Galaxy
        else:
            if gi:
                histories = gi.histories.get_histories()
                hid = histories[0]['id'] if histories else gi.histories.create_history("GPT-OSS Analysis")['id']
                ret = gi.tools.upload_file(temp_path, hid)
                result["type"] = "data"
                result["file_id"] = ret['outputs'][0]['id']
                result["suggestions"] = ["å¯¹è¿™ä¸ªæ–‡ä»¶åšè´¨æ§", "æŸ¥çœ‹æ–‡ä»¶å‰10è¡Œ", "æ¯”å¯¹åˆ°å‚è€ƒåŸºå› ç»„"]
                os.remove(temp_path)
            else:
                return {"status": "error", "message": "Galaxy æœªè¿æ¥"}
        return result
    except Exception as e:
        traceback.print_exc()
        return {"status": "error", "message": str(e)}

@app.post("/api/chat")
async def chat_handler(req: ChatRequest):
    file_ctx = {"has_file": bool(req.uploaded_file_id), "file_id": req.uploaded_file_id, "file_name": req.uploaded_file_name}
    response = agent.smart_process(req.message, file_ctx, req.history, req.selected_tool)
    return response

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=PORT)
