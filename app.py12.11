import os
import io
import json
import base64
import traceback
import contextlib
import uvicorn
import shutil
import time
import re
from fastapi import FastAPI, Request, UploadFile, File
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
from typing import List, Optional, Union, Dict, Any
from dotenv import load_dotenv

# --- LangChain & BioBlend ---
from bioblend.galaxy import GalaxyInstance
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage

load_dotenv()

# ================= é…ç½®åŒºåŸŸ =================
PORT = 8082
EMBED_MODEL = "nomic-embed-text"
LLM_MODEL = "gpt-oss:latest"
VISION_MODEL = "llama3.2-vision:11b"
OLLAMA_URL = "http://localhost:11434"
VECTOR_DB_PATH = "./data/chroma_db_bioblend"
GALAXY_URL = os.getenv("GALAXY_URL", "https://usegalaxy.org")
GALAXY_KEY = os.getenv("GALAXY_API_KEY", "")

# --- ã€æ ¸å¿ƒã€‘æ ‡å‡†å·¥ä½œæµå®šä¹‰ (ç™½åå•) ---
# åªæœ‰è¿™é‡Œåˆ—å‡ºçš„å·¥å…·æ‰ä¼šè¢«è§„åˆ’ï¼Œé˜²æ­¢ AI ä¹±åŠ  Salmon ä¹‹ç±»çš„å·¥å…·
WORKFLOW_TEMPLATES = {
    "seurat_standard": {
        "name": "Seurat å•ç»†èƒæ ‡å‡†åˆ†ææµç¨‹",
        "steps": [
            {"name": "1. åˆ›å»ºå¯¹è±¡ (Create)", "tool_id": "seurat_create_seurat_object", "desc": "å¯¼å…¥ 10x æ•°æ®ç”Ÿæˆ RDS"},
            {"name": "2. ç»†èƒè¿‡æ»¤ (Filter)", "tool_id": "seurat_filter_cells", "desc": "æ ¹æ®åŸºå› æ•°/çº¿ç²’ä½“æ¯”ä¾‹è¿‡æ»¤"},
            {"name": "3. å½’ä¸€åŒ– (Normalize)", "tool_id": "seurat_normalise_data", "desc": "LogNormalize æ ‡å‡†åŒ–"},
            {"name": "4. é«˜å˜åŸºå›  (FindVarGenes)", "tool_id": "seurat_find_variable_genes", "desc": "å¯»æ‰¾é«˜å˜å¼‚ç‰¹å¾"},
            {"name": "5. æ•°æ®ç¼©æ”¾ (ScaleData)", "tool_id": "seurat_scale_data", "desc": "çº¿æ€§ç¼©æ”¾æ•°æ®"},
            {"name": "6. PCA é™ç»´ (RunPCA)", "tool_id": "seurat_run_pca", "desc": "ä¸»æˆåˆ†åˆ†æ"},
            {"name": "7. æ„å»ºé‚»æ¥å›¾ (FindNeighbors)", "tool_id": "seurat_find_neighbours", "desc": "æ„å»º SNN å›¾"},
            {"name": "8. ç»†èƒèšç±» (FindClusters)", "tool_id": "seurat_find_clusters", "desc": "Louvain èšç±»"},
            {"name": "9. UMAP å¯è§†åŒ– (RunUMAP)", "tool_id": "seurat_run_umap", "desc": "éçº¿æ€§é™ç»´å¯è§†åŒ–"},
            {"name": "10. å·®å¼‚åˆ†æ (FindMarkers)", "tool_id": "seurat_find_markers", "desc": "å¯»æ‰¾ Cluster Marker åŸºå› "}
        ]
    }
}

# --- å¤æ‚å·¥å…·çš„ç¡¬æ€§éœ€æ±‚é…ç½® ---
TOOL_REQUIREMENTS = {
    "seurat_create": {
        "required_keywords": ["matrix", "genes", "barcodes"], 
        "min_files": 3,
        "guide_msg": "æ£€æµ‹åˆ° Seurat åˆå§‹åŒ–è¯·æ±‚ï¼Œæ­£åœ¨åº”ç”¨æ ‡å‡†å‚æ•°æ¨¡æ¿...",
        "code_template": """
# 1. è·å–å†å² ID
histories = gi.histories.get_histories()
history_id = histories[0]['id']

# 2. åŠ¨æ€æŸ¥æ‰¾å·¥å…· ID
tools = gi.tools.get_tools(name='Seurat Create')
target_tool = next((t for t in tools if 'seurat_create' in t['id']), tools[0])
tool_id = target_tool['id']

# 3. æ„å»ºå‚æ•°
tool_inputs = {{
    'method|method': 'CreateSeuratObject',
    'method|input_type|input_type': 'mtx',
    'method|input_type|matrix': {{'src': 'hda', 'id': '{matrix_id}'}},
    'method|input_type|gene_names': {{'src': 'hda', 'id': '{genes_id}'}},
    'method|input_type|cell_barcodes': {{'src': 'hda', 'id': '{barcodes_id}'}}
}}

print(f"æ­£åœ¨æäº¤ä»»åŠ¡ï¼Œå·¥å…·ID: {{tool_id}}")

# 4. æ‰§è¡Œ
ret = gi.tools.run_tool(tool_id=tool_id, tool_inputs=tool_inputs, history_id=history_id)

# 5. æ‰“å°ç»“æœ
print("çŠ¶æ€: âœ… ä»»åŠ¡å·²æˆåŠŸæäº¤ (Submitted)")
print(f"ä»»åŠ¡åç§°: Seurat Create Object")
if 'jobs' in ret and len(ret['jobs']) > 0:
    print(f"Galaxy Job ID: {{ret['jobs'][0]['id']}}")
else:
    print(f"Galaxy Job ID: (æœªè¿”å›æ˜¾å¼ Job ID)")

if 'outputs' in ret and len(ret['outputs']) > 0:
    print(f"è¾“å‡ºæ•°æ®é›†: {{ret['outputs'][0]['id']}}")
else:
    print(f"è¾“å‡ºæ•°æ®é›†: (è¯·åœ¨ Galaxy å†å²ä¸­æŸ¥çœ‹)")

print("æç¤º: è¯·å‰å¾€ Galaxy ç½‘é¡µç«¯æŸ¥çœ‹è¿è¡Œè¿›åº¦")
"""
    }
}
# ===========================================

gi = None
print(f">>> [ç³»ç»Ÿ] æ­£åœ¨è¿æ¥ Galaxy ({GALAXY_URL})...")
try:
    gi = GalaxyInstance(url=GALAXY_URL, key=GALAXY_KEY)
    user = gi.users.get_current_user()
    print(f">>> [ç³»ç»Ÿ] Galaxy è¿æ¥æˆåŠŸ! å½“å‰ç”¨æˆ·: {user.get('username', 'Unknown')}")
except Exception as e:
    print(f">>> [ä¸¥é‡é”™è¯¯] Galaxy è¿æ¥å¤±è´¥: {e}")

class BioBlendAgent:
    def __init__(self):
        self.vector_db = self._load_db()
        self.brain = ChatOllama(model=LLM_MODEL, base_url=OLLAMA_URL, temperature=0.1)
        self.eye = ChatOllama(model=VISION_MODEL, base_url=OLLAMA_URL, temperature=0)
        
        self.system_capabilities = """
        [Source A: System Kernel (Management)]
        1. Get Current User Info: gi.users.get_current_user()
        2. List Histories: gi.histories.get_histories()
        3. Upload File: gi.tools.upload_file('path', history_id)
        """
        
        self.error_analysis_prompt = ChatPromptTemplate.from_template("""
        You are a Galaxy Bioinformatics Error Analyst. 
        ã€Contextã€‘ Error: "{error_msg}"
        ã€Taskã€‘ Analyze and provide a fix in Chinese.
        **IMPORTANT**: Explain reasoning inside <think> tags first.
        """)

    def _load_db(self):
        if os.path.exists(VECTOR_DB_PATH) and os.listdir(VECTOR_DB_PATH):
            embeddings = OllamaEmbeddings(model=EMBED_MODEL, base_url=OLLAMA_URL)
            return Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)
        return None

    def ocr_image(self, image_path):
        try:
            with open(image_path, "rb") as image_file:
                image_data = base64.b64encode(image_file.read()).decode("utf-8")
            message = HumanMessage(content=[
                {"type": "text", "text": "Extract text and describe this image in Chinese."},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
            ])
            res = self.eye.invoke([message])
            return res.content
        except Exception as e:
            return f"OCR è¯†åˆ«å¤±è´¥: {str(e)}"

    def _analyze_domain(self, query):
        template = """
        You are a classifier. Output ONLY one word: "GALAXY_BIO" or "GENERAL".
        User Input: "{query}"
        """
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.brain | StrOutputParser()
        try:
            return chain.invoke({"query": query[:1000]}).strip()
        except:
            return "GALAXY_BIO"

    # ã€æ ¸å¿ƒä¿®å¤ã€‘è·¯ç”±é€»è¾‘é‡æ„ï¼šè§„åˆ’ > æ‰§è¡Œ
    def _classify_bio_intent(self, query):
        query_lower = query.lower()
        
        # 1. è§„åˆ’/é—®ç­”æ¨¡å¼ (ä¼˜å…ˆçº§æœ€é«˜)
        # åªè¦åŒ…å«è¿™äº›è¯ï¼Œå“ªæ€•æœ‰æ–‡ä»¶åï¼Œä¹Ÿå…ˆå‡ºæ–¹æ¡ˆ
        if any(k in query_lower for k in ["è§„åˆ’", "plan", "å…¨æµç¨‹", "pipeline", "workflow", "é¡ºåº", "order", "æ­¥éª¤", "steps", "æ€ä¹ˆ", "how to"]):
            print(f"   [è·¯ç”±] å…³é”®è¯å‘½ä¸­ -> PLANNING (å·¥ä½œæµè§„åˆ’)")
            return "WORKFLOW_PLANNING"
            
        # 2. æ‰§è¡Œæ¨¡å¼
        if any(k in query_lower for k in ["è´¦æˆ·", "account", "user", "ç”¨æˆ·", "history", "å†å²", "run", "è¿è¡Œ", "æ‰§è¡Œ", "upload", "ä¸Šä¼ ", "åš", "å®Œæˆ", "create"]):
            print(f"   [è·¯ç”±] å…³é”®è¯å‘½ä¸­ -> EXECUTION (æ‰§è¡Œ/ç®¡ç†)")
            return "EXECUTION"
            
        # 3. å’¨è¯¢æ¨¡å¼ (æŸ¥å·¥å…·)
        if any(k in query_lower for k in ["åˆ—å‡º", "list", "find", "search", "æŸ¥æ‰¾", "å·¥å…·", "tools", "æ¨è"]):
            print(f"   [è·¯ç”±] å…³é”®è¯å‘½ä¸­ -> CONSULTATION (å’¨è¯¢/æ£€ç´¢)")
            return "CONSULTATION"
            
        return "WORKFLOW_PLANNING" # é»˜è®¤å…œåº•

    def _identify_tool_key(self, tool_name):
        name = tool_name.lower()
        if "seurat" in name and ("create" in name or "read" in name or "åˆ›å»º" in name or "è¯»å–" in name or "åˆå§‹åŒ–" in name):
            return "seurat_create"
        return "unknown"

    def _fetch_galaxy_tool_params(self, tool_id):
        try:
            histories = gi.histories.get_histories()
            history_id = histories[0]['id'] if histories else gi.histories.create_history("Temp")['id']
            tool_info = gi.tools.build(tool_id=tool_id, history_id=history_id)
            extracted_params = []
            
            def recurse_inputs(inputs, context_path=""):
                for param in inputs:
                    if param.get('hidden', False): continue
                    if param['model_class'] == 'DataToolParameter': continue 
                    if param['model_class'] == 'Repeat':
                        recurse_inputs(param['inputs'], f"{context_path}[{param['title']}] ")
                        continue
                    if param['model_class'] == 'Conditional':
                        for case in param.get('cases', []):
                            recurse_inputs(case['inputs'], f"{context_path}[è‹¥ {param['name']}={case['value']}] ")
                        continue
                    if param['model_class'] == 'Section':
                        recurse_inputs(param['inputs'], f"{context_path}[{param['title']}] ")
                        continue
                    if param['type'] in ['integer', 'float', 'text', 'boolean', 'select']:
                        p_data = {
                            "name": param['name'],
                            "label": f"{context_path}{param.get('label', param['name'])}",
                            "type": param['type'],
                            "value": param.get('value', ''),
                            "help": param.get('help', '')
                        }
                        if param['type'] == 'select':
                            p_data['options'] = [{"label": o[0], "value": o[1]} for o in param.get('options', []) if len(o)>=2]
                        extracted_params.append(p_data)

            recurse_inputs(tool_info['inputs'])
            return extracted_params
        except Exception as e:
            print(f"Error fetching params: {e}")
            return None

    def smart_process(self, user_query, file_context_list, chat_history=[], selected_tool=None, tool_params=None, workflow_data=None):
        if not self.vector_db:
            return {"type": "text", "reply": "âŒ é”™è¯¯ï¼šçŸ¥è¯†åº“æœªåŠ è½½ã€‚", "suggestions": []}

        # 0. å¼ºåˆ¶æ‰§è¡Œé€šé“
        if selected_tool:
            if not tool_params:
                return self._handle_tool_selection(selected_tool, file_context_list)
            else:
                return self._generate_code_with_params(selected_tool, tool_params, file_context_list)
        
        # 0.1 å·¥ä½œæµæ‰§è¡Œé€šé“
        if workflow_data:
            return self._execute_workflow_chain(workflow_data, file_context_list)

        # 1. é¢†åŸŸæ„ŸçŸ¥
        domain = self._analyze_domain(user_query)

        if "GENERAL" in domain:
            template = """
            You are a helpful AI Assistant. User Input: {query}. History: {history}. 
            Answer directly in Chinese.
            **IMPORTANT**: First, think step-by-step inside <think> tags.
            """
            history_text = "\n".join([f"User: {h.get('user','')}\nAI: {h.get('ai','')}" for h in chat_history[-3:]])
            prompt = ChatPromptTemplate.from_template(template)
            chain = prompt | self.brain | StrOutputParser()
            response = chain.invoke({"query": user_query, "history": history_text})
            return self._parse_llm_response(response, context="chat", user_query=user_query)

        else:
            bio_intent = self._classify_bio_intent(user_query)
            history_text = "\n".join([f"User: {h.get('user','')}\nAI: {h.get('ai','')}" for h in chat_history[-3:]])

            if bio_intent == "WORKFLOW_PLANNING":
                # === å·¥ä½œæµè§„åˆ’æ¨¡å¼ ===
                if "seurat" in user_query.lower() or "å•ç»†èƒ" in user_query or "è½¬å½•ç»„" in user_query:
                    template_key = "seurat_standard"
                    wf = WORKFLOW_TEMPLATES[template_key]
                    
                    steps_with_params = []
                    print(f"   [ç³»ç»Ÿ] æ­£åœ¨ä¸ºå·¥ä½œæµæŠ“å–å‚æ•°...")
                    for step in wf['steps']:
                        # å°è¯•è·å–å‚æ•°ï¼Œå¦‚æœå¤±è´¥åˆ™åªè¿”å›åŸºæœ¬ä¿¡æ¯
                        res = self._fetch_galaxy_tool_params(step['tool_id'])
                        params = res if res else []
                        
                        steps_with_params.append({
                            "name": step['name'],
                            "tool_id": step['tool_id'], 
                            "desc": step['desc'],
                            "params": params
                        })
                    
                    return {
                        "type": "workflow_config",
                        "workflow_name": wf['name'],
                        "steps": steps_with_params,
                        "reply": f"å·²ä¸ºæ‚¨è§„åˆ’ **{wf['name']}**ï¼ŒåŒ…å« {len(steps_with_params)} ä¸ªæ­¥éª¤ã€‚è¯·ç¡®è®¤æ¯ä¸€æ­¥çš„å‚æ•°ï¼š",
                        "thought": "<think>è¯†åˆ«åˆ°ç”¨æˆ·éœ€è¦å•ç»†èƒå…¨æµç¨‹ã€‚å·²åŠ è½½ Seurat æ ‡å‡†æ¨¡æ¿ï¼Œå¹¶å®æ—¶æŠ“å–äº†æ‰€æœ‰å·¥å…·çš„æœ€æ–°å‚æ•°é…ç½®ã€‚</think>"
                    }

            if bio_intent == "CONSULTATION":
                retriever = self.vector_db.as_retriever(search_kwargs={"k": 25})
                docs = retriever.invoke(user_query)
                retrieved_tools = "\n".join([f"- {d.page_content}" for i, d in enumerate(docs)])

                template = """
                You are a Galaxy Tool Consultant. User Request: "{query}"
                ã€Available Tools (RAG)ã€‘ {retrieved_tools}
                **TASK**: List tools in JSON format.
                **MANDATORY**: Start with <think> tags. Output JSON: ```json [ {{"name": "...", "id": "..."}}, ... ] ```
                """
                prompt = ChatPromptTemplate.from_template(template)
                chain = prompt | self.brain | StrOutputParser()
                response = chain.invoke({"query": user_query, "retrieved_tools": retrieved_tools})
                
            else:
                # EXECUTION
                tool_key = self._identify_tool_key(user_query)
                if tool_key in TOOL_REQUIREMENTS and 'code_template' in TOOL_REQUIREMENTS[tool_key]:
                    template_data = TOOL_REQUIREMENTS[tool_key]
                    
                    matrix_id = next((f['id'] for f in file_context_list if 'matrix' in f['name']), "MISSING")
                    genes_id = next((f['id'] for f in file_context_list if 'genes' in f['name'] or 'features' in f['name']), "MISSING")
                    barcodes_id = next((f['id'] for f in file_context_list if 'barcodes' in f['name']), "MISSING")
                    
                    if "MISSING" in [matrix_id, genes_id, barcodes_id]:
                        return {"type": "text", "reply": template_data['guide_msg'] + f"\nå½“å‰æ–‡ä»¶çŠ¶æ€: Matrix={matrix_id}, Genes={genes_id}, Barcodes={barcodes_id}", "thought": "<think>æ£€æµ‹åˆ°æ–‡ä»¶ç¼ºå¤±ï¼Œä¸­æ–­æ‰§è¡Œã€‚</think>", "suggestions": []}
                    
                    final_code = template_data['code_template'].format(
                        matrix_id=matrix_id,
                        genes_id=genes_id,
                        barcodes_id=barcodes_id
                    )
                    
                    response = f"<think>æ£€æµ‹åˆ°ç”¨æˆ·æ„å›¾ä¸ºè¿è¡Œ Seuratã€‚åŒ¹é…åˆ°æ ‡å‡†æ¨¡æ¿ã€‚å·²è‡ªåŠ¨æå–æ–‡ä»¶ ID å¹¶ç”Ÿæˆä»£ç ã€‚</think>\n```python\n{final_code}\n```"
                
                else:
                    retriever = self.vector_db.as_retriever(search_kwargs={"k": 3})
                    docs = retriever.invoke(user_query)
                    retrieved_knowledge = "\n".join([f"- {d.page_content}" for i, d in enumerate(docs)])

                    template = """
                    You are a Galaxy System Executor. User Request: "{query}"
                    ã€System Capabilitiesã€‘ {system_caps}
                    ã€Knowledge Baseã€‘ {retrieved_knowledge}
                    ã€Contextã€‘ Uploaded Files: {file_context_list}
                    **TASK**: Write Python code using `bioblend`.
                    **CRITICAL**: Use `gi` directly. NO `if __name__`. Print results.
                    **MANDATORY**: Start with <think> tags. Output Python Code: ```python ... ```
                    """
                    prompt = ChatPromptTemplate.from_template(template)
                    chain = prompt | self.brain | StrOutputParser()
                    response = chain.invoke({
                        "query": user_query,
                        "system_caps": self.system_capabilities,
                        "retrieved_knowledge": retrieved_knowledge,
                        "file_context_list": str(file_context_list),
                        "history": history_text
                    })

            return self._parse_llm_response(response, context="chat", user_query=user_query)

    def _handle_tool_selection(self, tool_info, file_context_list):
        tool_name = tool_info['name']
        tool_id = tool_info.get('id', '')
        
        if not tool_id or "unknown" in tool_id.lower():
            try:
                tools = gi.tools.get_tools(name=tool_name)
                if tools: tool_id = tools[0]['id']
            except: pass
            
        print(f"   [ç³»ç»Ÿ] æ­£åœ¨æŠ“å–å·¥å…·å‚æ•°: {tool_name} ({tool_id})")
        params = self._fetch_galaxy_tool_params(tool_id)
        
        if params:
            params_json = json.dumps(params, indent=2, ensure_ascii=False)
            return {
                "type": "tool_config",
                "tool_name": tool_name,
                "tool_id": tool_id,
                "params": params,
                "reply": f"å·²è·å– **{tool_name}** çš„é…ç½®å‚æ•°ï¼Œè¯·ç¡®è®¤ï¼š",
                "thought": f"<think>æˆåŠŸè°ƒç”¨ Galaxy API è·å–äº†å·¥å…·å‚æ•°ç»“æ„ã€‚å…±å‘ç° {len(params)} ä¸ªå¯é…ç½®å‚æ•°ã€‚</think>"
            }
        else:
            return self._generate_code_only(f"Run {tool_name}", file_context_list, tool_info)

    def _execute_workflow_chain(self, workflow_data, file_context_list):
        steps = workflow_data['steps']
        execution_log = []
        last_output_id = None
        
        histories = gi.histories.get_histories()
        history_id = histories[0]['id']
        
        matrix_id = next((f['id'] for f in file_context_list if 'matrix' in f['name']), None)
        genes_id = next((f['id'] for f in file_context_list if 'genes' in f['name'] or 'features' in f['name']), None)
        barcodes_id = next((f['id'] for f in file_context_list if 'barcodes' in f['name']), None)

        print(f"   [å·¥ä½œæµ] å¼€å§‹æ‰§è¡Œ {len(steps)} ä¸ªæ­¥éª¤...")
        
        for i, step in enumerate(steps):
            tool_id = step['tool_id']
            user_params = step['params']
            
            print(f"   [Step {i+1}] Running {step['name']}...")
            
            try:
                tool_inputs = {}
                # å¡«å…¥ç”¨æˆ·å‚æ•°
                if isinstance(user_params, list): # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œè¯´æ˜æ˜¯å‚æ•°å®šä¹‰ï¼Œä¸æ˜¯å€¼
                     # è¿™é‡Œç®€åŒ–ï¼šå¦‚æœæ˜¯åˆ—è¡¨ï¼Œè¯´æ˜ç”¨æˆ·æ²¡æ”¹ï¼Œç”¨é»˜è®¤å€¼ï¼ˆè¿™é‡Œéœ€è¦å‰ç«¯ä¼ å›å…·ä½“å€¼ï¼Œæš‚ç•¥ï¼‰
                     pass
                elif isinstance(user_params, dict):
                    for k, v in user_params.items():
                        tool_inputs[k] = v
                
                if i == 0: 
                    if "create" in tool_id or "read10x" in tool_id:
                        if not (matrix_id and genes_id and barcodes_id):
                             return {"type": "text", "reply": f"âŒ æ­¥éª¤ 1 å¤±è´¥ï¼šç¼ºå°‘å¿…è¦çš„ 10x æ–‡ä»¶ (Matrix/Genes/Barcodes)ã€‚", "thought": None}
                        
                        tool_inputs['method|method'] = 'CreateSeuratObject'
                        tool_inputs['method|input_type|input_type'] = 'mtx'
                        tool_inputs['method|input_type|matrix'] = {'src': 'hda', 'id': matrix_id}
                        tool_inputs['method|input_type|gene_names'] = {'src': 'hda', 'id': genes_id}
                        tool_inputs['method|input_type|cell_barcodes'] = {'src': 'hda', 'id': barcodes_id}
                else:
                    # è‡ªåŠ¨æ¥åŠ›
                    tool_inputs['seurat_rds'] = {'src': 'hda', 'id': last_output_id}
                
                ret = gi.tools.run_tool(tool_id=tool_id, tool_inputs=tool_inputs, history_id=history_id)
                
                if ret['outputs']:
                    last_output_id = ret['outputs'][0]['id']
                    execution_log.append(f"âœ… Step {i+1} {step['name']}: Job {ret['jobs'][0]['id']} Submitted.")
                else:
                    execution_log.append(f"âš ï¸ Step {i+1} {step['name']}: No output produced.")
                    
            except Exception as e:
                execution_log.append(f"âŒ Step {i+1} {step['name']} Failed: {str(e)}")
                break 
        
        final_report = "\n".join(execution_log)
        return {
            "type": "text",
            "reply": f"**å·¥ä½œæµæ‰§è¡ŒæŠ¥å‘Š**\n\n{final_report}",
            "thought": "<think>å·¥ä½œæµå¼•æ“å·²æŒ‰é¡ºåºæäº¤ä»»åŠ¡ã€‚æ•°æ®æµå·²è‡ªåŠ¨æ¥åŠ›ã€‚</think>",
            "suggestions": ["æŸ¥çœ‹ Galaxy å†å²", "ä¸‹è½½ç»“æœ"]
        }

    def _generate_code_with_params(self, tool_info, user_params, file_context_list):
        files_desc = "\n".join([f"- ID: {f['id']} | Filename: {f['name']}" for f in file_context_list])
        params_json = json.dumps(user_params, indent=2, ensure_ascii=False)
        
        template = """
        You are a Galaxy BioBlend Expert. 
        Target Tool: "{tool_name}" (ID: {tool_id})
        ã€User Configured Parametersã€‘ {params_json}
        ã€Available Filesã€‘ {files_desc}
        **TASK**: Write Python code to run this tool.
        **CRITICAL**: 
        1. Use `gi.tools.run_tool`. 
        2. **INPUT CHAINING**: Find the latest dataset in history to use as input (if required).
        3. Map user parameters to `tool_inputs`.
        4. Start with <think> tags.
        """
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.brain | StrOutputParser()
        response = chain.invoke({
            "tool_name": tool_info['name'],
            "tool_id": tool_info['id'],
            "params_json": params_json,
            "files_desc": files_desc
        })
        return self._parse_llm_response(response, context="execution")

    def _generate_code_only(self, query, file_context_list, tool_info):
        files_desc = "\n".join([f"- ID: {f['id']} | Filename: {f['name']}" for f in file_context_list])
        template = """
        You are a Galaxy BioBlend Expert. User selected tool "{tool_name}" (ID: {tool_id}).
        ã€Available Filesã€‘ {files_desc}
        **CRITICAL**: Use `gi`. No `if __name__`.
        **MANDATORY**: Start with <think> tags. Output Python Code.
        """
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.brain | StrOutputParser()
        response = chain.invoke({
            "tool_name": tool_info['name'],
            "tool_id": tool_info['id'],
            "query": query,
            "files_desc": files_desc
        })
        return self._parse_llm_response(response, context="execution", user_query=query)

    def _extract_thought(self, text):
        pattern = r"<think>(.*?)</think>"
        match = re.search(pattern, text, re.DOTALL)
        if match:
            thought = match.group(1).strip()
            clean_text = re.sub(pattern, "", text, flags=re.DOTALL).strip()
            return thought, clean_text
        else:
            return None, text

    def _parse_llm_response(self, response, context="chat", user_query=""):
        response = response.strip()
        thought_content, clean_response = self._extract_thought(response)
        
        if "```python" in clean_response:
            try:
                code = clean_response.split("```python")[1].split("```")[0].strip()
                success, exec_out, error_msg = self._execute_code_sandbox(code)
                
                if success:
                    final_reply = f"### ğŸ¤– ç­–ç•¥ä»£ç \n```python\n{code}\n```\n### âœ… æ‰§è¡Œç»“æœ\n```text\n{exec_out}\n```"
                    return {"type": "text", "reply": final_reply, "thought": thought_content, "suggestions": ["ä¿å­˜ç»“æœ", "å¯è§†åŒ–"]}
                else:
                    print(f"   [é”™è¯¯] ä»£ç æ‰§è¡Œå¤±è´¥: {error_msg}")
                    diagnosis = self._analyze_error_and_guide(user_query, code, error_msg)
                    return {"type": "text", "reply": diagnosis, "thought": thought_content, "suggestions": ["é‡è¯•"]}
            except:
                 return {"type": "text", "reply": clean_response, "thought": thought_content, "suggestions": []}
        
        elif "```json" in clean_response:
            return self._process_json_candidates(clean_response.split("```json")[1].split("```")[0].strip(), thought_content)
        elif "[" in clean_response and "]" in clean_response:
            try:
                start = clean_response.find("[")
                end = clean_response.rfind("]") + 1
                potential_json = clean_response[start:end]
                json.loads(potential_json) 
                return self._process_json_candidates(potential_json, thought_content)
            except:
                return {"type": "text", "reply": clean_response, "thought": thought_content, "suggestions": []}
        
        else:
            return {"type": "text", "reply": clean_response, "thought": thought_content, "suggestions": []}

    def _process_json_candidates(self, json_str, thought_content):
        try:
            candidates = json.loads(json_str)
            if not isinstance(candidates, list): candidates = [candidates]
            cleaned_candidates = []
            for c in candidates:
                if isinstance(c, str):
                    if "unknown" in c.lower(): continue
                    c = {"name": c, "id": ""}
                elif isinstance(c, dict):
                    name = c.get('name', 'Unknown Tool')
                    raw_id = c.get('id', '')
                    if "unknown" in name.lower() or "unknown" in raw_id.lower(): continue
                    c['id'] = raw_id
                cleaned_candidates.append(c)
            
            if not cleaned_candidates:
                 return {"type": "text", "reply": "æœªæ‰¾åˆ°ç›¸å…³å·¥å…·ã€‚", "thought": thought_content, "suggestions": []}

            return {"type": "choice", "reply": f"ä¸ºæ‚¨æ‰¾åˆ°äº† {len(cleaned_candidates)} ä¸ªç›¸å…³å·¥å…·ï¼š", "candidates": cleaned_candidates, "thought": thought_content, "suggestions": []}
        except:
            return {"type": "text", "reply": json_str, "thought": thought_content, "suggestions": []}

    def _analyze_error_and_guide(self, query, code, error_msg):
        chain = self.error_analysis_prompt | self.brain | StrOutputParser()
        try:
            short_error = error_msg[-2000:] if len(error_msg) > 2000 else error_msg
            diagnosis = chain.invoke({"query": query, "code_snippet": code[:500], "error_msg": short_error})
            thought, clean_diag = self._extract_thought(diagnosis)
            return clean_diag 
        except Exception as e:
            return f"âŒ æ‰§è¡Œå‡ºé”™ã€‚\nåŸå§‹é”™è¯¯: {str(e)}"

    def _execute_code_sandbox(self, code):
        if not gi: return False, "", "Galaxy æœªè¿æ¥"
        output_buffer = io.StringIO()
        try:
            sandbox = {"gi": gi, "json": json, "print": print}
            with contextlib.redirect_stdout(output_buffer):
                exec(code, sandbox)
            return True, output_buffer.getvalue() or "(æ‰§è¡ŒæˆåŠŸï¼Œæ— å±å¹•è¾“å‡º)", ""
        except Exception:
            err = traceback.format_exc()
            return False, "", err

app = FastAPI()
templates = Jinja2Templates(directory="templates")
agent = BioBlendAgent()

class ChatRequest(BaseModel):
    message: str
    history: List[Dict[str, str]] = [] 
    selected_tool: Optional[Dict[str, Any]] = None
    tool_params: Optional[Dict[str, Any]] = None
    workflow_data: Optional[Dict[str, Any]] = None # æ–°å¢ï¼šå·¥ä½œæµæ•°æ®
    uploaded_files: List[Dict[str, str]] = [] 

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/api/upload")
async def upload_handler(file: UploadFile = File(...)):
    try:
        temp_path = f"temp_{file.filename}"
        with open(temp_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        result = {"status": "success", "file_name": file.filename}
        if file.filename.lower().endswith(('.png', '.jpg', '.jpeg')):
            ocr_text = agent.ocr_image(temp_path)
            result["type"] = "image"
            result["ocr_text"] = ocr_text
            result["suggestions"] = []
            os.remove(temp_path) 
        else:
            if gi:
                histories = gi.histories.get_histories()
                hid = histories[0]['id'] if histories else gi.histories.create_history("GPT-OSS Analysis")['id']
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        print(f"Uploading {file.filename} (Attempt {attempt+1}/{max_retries})...")
                        ret = gi.tools.upload_file(temp_path, hid)
                        result["type"] = "data"
                        result["file_id"] = ret['outputs'][0]['id']
                        result["suggestions"] = []
                        break 
                    except Exception as upload_err:
                        print(f"Upload failed: {upload_err}")
                        if attempt < max_retries - 1: time.sleep(2) 
                        else: raise upload_err
                os.remove(temp_path)
            else:
                return {"status": "error", "message": "Galaxy æœªè¿æ¥"}
        return result
    except Exception as e:
        return {"status": "error", "message": f"ä¸Šä¼ å¤±è´¥: {str(e)}"}

@app.post("/api/chat")
async def chat_handler(req: ChatRequest):
    try:
        response = agent.smart_process(req.message, req.uploaded_files, req.history, req.selected_tool, req.tool_params, req.workflow_data)
        return response
    except Exception as e:
        traceback.print_exc()
        return JSONResponse(status_code=500, content={"type": "text", "reply": f"âŒ ç³»ç»Ÿå†…éƒ¨é”™è¯¯: {str(e)}", "thought": None, "suggestions": ["é‡è¯•"]})

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=PORT)
