import os
import io
import json
import base64
import traceback
import contextlib
import uvicorn
import shutil
import time
import re
import ast
import threading
import uuid
from datetime import datetime
from fastapi import FastAPI, Request, UploadFile, File, BackgroundTasks
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
from typing import List, Optional, Union, Dict, Any
from dotenv import load_dotenv
from bioblend import ConnectionError 

# --- LangChain & BioBlend ---
from bioblend.galaxy import GalaxyInstance
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage

load_dotenv()

# ================= é…ç½®åŒºåŸŸ =================
PORT = 8082
EMBED_MODEL = "nomic-embed-text"
LLM_MODEL = "qwen3-vl:32b" 
OLLAMA_URL = "http://localhost:11434"
VECTOR_DB_PATH = "./data/chroma_db_bioblend"
GALAXY_URL = os.getenv("GALAXY_URL", "https://usegalaxy.org")
GALAXY_KEY = os.getenv("GALAXY_API_KEY", "")

# --- å·¥ä½œæµå®šä¹‰ ---
WORKFLOW_TEMPLATES = {
    "seurat_standard": {
        "name": "Seurat å•ç»†èƒæ ‡å‡†åˆ†ææµç¨‹",
        "steps": [
            {"name": "1. åˆ›å»ºå¯¹è±¡ (Create)", "tool_keyword": "Seurat Create", "desc": "å¯¼å…¥ 10x æ•°æ®ç”Ÿæˆ RDS", "params": {"min_cells": "0", "min_features": "0", "project_name": "Project1"}},
            {"name": "2. ç»†èƒè¿‡æ»¤ (Filter)", "tool_keyword": "Seurat FilterCells", "desc": "æ ¹æ®åŸºå› æ•°/çº¿ç²’ä½“æ¯”ä¾‹è¿‡æ»¤", "params": {"subset_name": "nFeature_RNA", "low_threshold": "0", "high_threshold": "1000000000"}},
            {"name": "3. å½’ä¸€åŒ– (Normalize)", "tool_keyword": "Seurat NormaliseData", "desc": "LogNormalize æ ‡å‡†åŒ–", "params": {"normalization_method": "LogNormalize", "scale_factor": "10000"}},
            {"name": "4. é«˜å˜åŸºå›  (FindVarGenes)", "tool_keyword": "Seurat FindVariableGenes", "desc": "å¯»æ‰¾é«˜å˜å¼‚ç‰¹å¾", "params": {"selection_method": "vst", "nfeatures": "2000"}},
            {"name": "5. æ•°æ®ç¼©æ”¾ (ScaleData)", "tool_keyword": "Seurat ScaleData", "desc": "çº¿æ€§ç¼©æ”¾æ•°æ®", "params": {"do_center": "true", "do_scale": "true"}},
            {"name": "6. PCA é™ç»´ (RunPCA)", "tool_keyword": "Seurat RunPCA", "desc": "ä¸»æˆåˆ†åˆ†æ", "params": {"npcs": "50"}},
            {"name": "7. æ„å»ºé‚»æ¥å›¾ (FindNeighbors)", "tool_keyword": "Seurat FindNeighbours", "desc": "æ„å»º SNN å›¾", "params": {"dims": "1:10", "k_param": "20"}},
            {"name": "8. ç»†èƒèšç±» (FindClusters)", "tool_keyword": "Seurat FindClusters", "desc": "Louvain èšç±»", "params": {"resolution": "0.5", "algorithm": "1"}},
            {"name": "9. UMAP å¯è§†åŒ– (RunUMAP)", "tool_keyword": "Seurat Run UMAP", "desc": "éçº¿æ€§é™ç»´å¯è§†åŒ–", "params": {"dims": "1:10"}},
            {"name": "10. å·®å¼‚åˆ†æ (FindMarkers)", "tool_keyword": "Seurat FindMarkers", "desc": "å¯»æ‰¾ Cluster Marker åŸºå› ", "params": {"min_pct": "0.25", "logfc_threshold": "0.25"}}
        ]
    }
}

TOOL_REQUIREMENTS = {
    "seurat_create": {
        "required_keywords": ["matrix", "genes", "barcodes"], 
        "min_files": 3,
        "guide_msg": "æ£€æµ‹åˆ° Seurat åˆå§‹åŒ–è¯·æ±‚ï¼Œæ­£åœ¨åº”ç”¨æ ‡å‡†å‚æ•°æ¨¡æ¿..."
    }
}

gi = None
print(f">>> [ç³»ç»Ÿ] æ­£åœ¨è¿æ¥ Galaxy ({GALAXY_URL})...")
try:
    gi = GalaxyInstance(url=GALAXY_URL, key=GALAXY_KEY)
    user = gi.users.get_current_user()
    print(f">>> [ç³»ç»Ÿ] Galaxy è¿æ¥æˆåŠŸ! å½“å‰ç”¨æˆ·: {user.get('username', 'Unknown')}")
except Exception as e:
    print(f">>> [ä¸¥é‡é”™è¯¯] Galaxy è¿æ¥å¤±è´¥: {e}")

# --- ä»»åŠ¡ç®¡ç†å™¨ ---
class WorkflowManager:
    def __init__(self):
        self.tasks = {} 

    def create_task(self, workflow_name, total_steps):
        run_id = str(uuid.uuid4())
        self.tasks[run_id] = {
            "status": "running",
            "workflow_name": workflow_name,
            "current_step_index": 0,
            "total_steps": total_steps,
            "steps_status": [{"name": "", "status": "pending"} for _ in range(total_steps)],
            "logs": [],
            "error": None,
            "completed": False
        }
        return run_id

    def update_step(self, run_id, step_index, step_name, status, log=None):
        if run_id in self.tasks:
            self.tasks[run_id]["steps_status"][step_index]["name"] = step_name
            self.tasks[run_id]["steps_status"][step_index]["status"] = status
            if status == "running":
                self.tasks[run_id]["current_step_index"] = step_index
            if log:
                self.tasks[run_id]["logs"].append(log)
                print(f"[Task {run_id}] {log}")

    def fail_task(self, run_id, error_msg):
        if run_id in self.tasks:
            self.tasks[run_id]["status"] = "failed"
            self.tasks[run_id]["error"] = error_msg
            self.tasks[run_id]["completed"] = True
            print(f"[Task {run_id}] FAILED: {error_msg}")

    def complete_task(self, run_id):
        if run_id in self.tasks:
            self.tasks[run_id]["status"] = "success"
            self.tasks[run_id]["completed"] = True
            print(f"[Task {run_id}] COMPLETED")

wf_manager = WorkflowManager()

class BioBlendAgent:
    def __init__(self):
        self.vector_db = self._load_db()
        self.brain = ChatOllama(model=LLM_MODEL, base_url=OLLAMA_URL, temperature=0.1)
        self.system_capabilities = """
        [Source A: System Kernel (Management)]
        1. Get Current User Info: gi.users.get_current_user()
        2. List Histories: gi.histories.get_histories()
        3. Upload File: gi.tools.upload_file('path', history_id)
        """
        self.error_analysis_prompt = ChatPromptTemplate.from_template("""
        You are a Galaxy Bioinformatics Error Analyst. 
        ã€Contextã€‘ Error: "{error_msg}"
        ã€Taskã€‘ Analyze and provide a fix in Chinese.
        **IMPORTANT**: Explain reasoning inside [THOUGHT] tags first.
        """)
        
        # ã€æ ¸å¿ƒæ–°å¢ã€‘è¯­ä¹‰è·¯ç”± Prompt
        self.router_prompt = ChatPromptTemplate.from_template("""
        You are an intent classifier for a Bioinformatics AI Agent.
        Analyze the User Input and Chat History to determine the user's intent.

        Categories:
        1. WORKFLOW_PLANNING: 
           - User wants to design, plan, or list steps for a workflow (e.g., "Seurat pipeline", "show steps").
           - User confirms to use history files for planning (e.g., "Yes", "Use these files").
           - User asks to "check" or "list" data files for the purpose of planning.
        2. EXECUTION: 
           - User explicitly asks to RUN, EXECUTE, or START a tool or workflow.
           - User confirms execution (e.g., "Run it", "Confirm").
           - User uploads files and implies execution.
        3. CONSULTATION: 
           - User asks to search, find, or list available TOOLS (not workflow steps).
           - e.g., "Find tools for PCA", "List all Seurat tools".
        4. GENERAL: 
           - Greetings, self-introduction, general biology questions, or unclear intent.

        Chat History:
        {history}

        User Input:
        {query}

        Output ONLY the category name (WORKFLOW_PLANNING, EXECUTION, CONSULTATION, or GENERAL). Do not output anything else.
        """)

    def _load_db(self):
        if os.path.exists(VECTOR_DB_PATH) and os.listdir(VECTOR_DB_PATH):
            try:
                embeddings = OllamaEmbeddings(model=EMBED_MODEL, base_url=OLLAMA_URL)
                return Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)
            except Exception as e:
                print(f"Loading Vector DB failed: {e}")
                return None
        return None

    def ocr_image(self, image_path):
        try:
            with open(image_path, "rb") as image_file:
                image_data = base64.b64encode(image_file.read()).decode("utf-8")
            message = HumanMessage(content=[
                {"type": "text", "text": "Extract text and describe this image in Chinese. If it's a plot, analyze the biological meaning."},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
            ])
            res = self.brain.invoke([message])
            return res.content
        except Exception as e:
            return f"OCR è¯†åˆ«å¤±è´¥: {str(e)}"

    def _analyze_domain(self, query):
        # è¿™ä¸ªæ–¹æ³•ç°åœ¨ä¸»è¦ç”¨äºçº¯é—²èŠåˆ¤æ–­ï¼Œæ ¸å¿ƒè·¯ç”±å·²ç§»è‡³ _classify_bio_intent
        return "GALAXY_BIO"

    def _classify_bio_intent(self, query, chat_history=[]):
        """ã€æ ¸å¿ƒå‡çº§ã€‘åŸºäº LLM çš„è¯­ä¹‰è·¯ç”±"""
        
        # æ„å»ºå†å²è®°å½•å­—ç¬¦ä¸²
        history_str = ""
        if chat_history:
            # å–æœ€è¿‘ 2 è½®å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡
            for msg in chat_history[-2:]:
                history_str += f"User: {msg.get('user', '')}\nAI: {msg.get('ai', '')}\n"
        
        try:
            # è°ƒç”¨ LLM è¿›è¡Œåˆ†ç±»
            chain = self.router_prompt | self.brain | StrOutputParser()
            intent = chain.invoke({"query": query, "history": history_str}).strip().upper()
            
            # æ¸…æ´— LLM å¯èƒ½è¾“å‡ºçš„å¤šä½™æ ‡ç‚¹
            intent = re.sub(r'[^\w_]', '', intent)
            
            print(f"   [è¯­ä¹‰è·¯ç”±] Input: '{query}' -> Intent: {intent}")
            
            valid_intents = ["WORKFLOW_PLANNING", "EXECUTION", "CONSULTATION", "GENERAL"]
            if intent in valid_intents:
                return intent
            else:
                print(f"   [è¯­ä¹‰è·¯ç”±] Unknown intent '{intent}', fallback to GENERAL")
                return "GENERAL"
                
        except Exception as e:
            print(f"   [è¯­ä¹‰è·¯ç”±] Error: {e}, fallback to keyword matching")
            # é™çº§ç­–ç•¥ï¼šå¦‚æœ LLM æŒ‚äº†ï¼Œå›é€€åˆ°ç®€å•çš„å…³é”®è¯åŒ¹é…
            query_lower = query.lower()
            if "run" in query_lower or "æ‰§è¡Œ" in query_lower: return "EXECUTION"
            if "plan" in query_lower or "è§„åˆ’" in query_lower: return "WORKFLOW_PLANNING"
            return "GENERAL"

    def _identify_tool_key(self, tool_name):
        name = tool_name.lower()
        if "seurat" in name and ("create" in name or "read" in name or "åˆ›å»º" in name or "è¯»å–" in name or "åˆå§‹åŒ–" in name):
            return "seurat_create"
        return "unknown"

    def _find_tool_id_by_keyword(self, keyword):
        try:
            tools = gi.tools.get_tools(name=keyword)
            if tools:
                print(f"   [Tool Search] Exact match '{keyword}': {tools[0]['id']}")
                return tools[0]['id']
            
            all_tools = gi.tools.get_tools()
            for t in all_tools:
                if keyword.lower() in t['name'].lower():
                    print(f"   [Tool Search] Fuzzy match '{keyword}' -> {t['name']}: {t['id']}")
                    return t['id']
                    
            print(f"   [Tool Search] Warning: No tool found for '{keyword}'")
            return None
        except Exception as e:
            print(f"   [Tool Search] Error searching '{keyword}': {e}")
            return None

    def _fetch_galaxy_tool_params(self, tool_keyword):
        tool_id = self._find_tool_id_by_keyword(tool_keyword)
        if not tool_id: return []
        
        try:
            histories = gi.histories.get_histories()
            history_id = histories[0]['id'] if histories else gi.histories.create_history("Temp")['id']
            tool_info = gi.tools.build(tool_id=tool_id, history_id=history_id)
            extracted_params = []
            
            def recurse_inputs(inputs, context_path=""):
                for param in inputs:
                    if param.get('hidden', False): continue
                    if param['model_class'] == 'DataToolParameter': continue 
                    if param['model_class'] == 'Repeat':
                        recurse_inputs(param['inputs'], f"{context_path}[{param['title']}] ")
                        continue
                    if param['model_class'] == 'Conditional':
                        for case in param.get('cases', []):
                            recurse_inputs(case['inputs'], f"{context_path}[è‹¥ {param['name']}={case['value']}] ")
                        continue
                    if param['model_class'] == 'Section':
                        recurse_inputs(param['inputs'], f"{context_path}[{param['title']}] ")
                        continue
                    if param['type'] in ['integer', 'float', 'text', 'boolean', 'select']:
                        p_data = {
                            "name": param['name'],
                            "label": f"{context_path}{param.get('label', param['name'])}",
                            "type": param['type'],
                            "value": param.get('value', ''),
                            "help": param.get('help', '')
                        }
                        if param['type'] == 'select':
                            p_data['options'] = [{"label": o[0], "value": o[1]} for o in param.get('options', []) if len(o)>=2]
                        extracted_params.append(p_data)

            recurse_inputs(tool_info['inputs'])
            return extracted_params
        except Exception as e:
            print(f"Error fetching params: {e}")
            return None

    def _get_history_datasets(self):
        try:
            histories = gi.histories.get_histories()
            if not histories: return []
            history_id = histories[0]['id']
            datasets = gi.histories.show_history(history_id, contents=True, deleted=False, visible=True)
            valid_datasets = []
            for d in datasets:
                if d['history_content_type'] == 'dataset' and d['state'] == 'ok':
                    valid_datasets.append({
                        "id": d['id'],
                        "hid": d['hid'], 
                        "name": d['name'],
                        "ext": d['extension'],
                        "size": d.get('file_size', 'Unknown')
                    })
            valid_datasets.sort(key=lambda x: x['hid'], reverse=True)
            return valid_datasets[:100] 
        except Exception as e:
            print(f"Fetch history failed: {e}")
            return []

    def _parse_files_from_query(self, query, history_files):
        selected_files = []
        query_lower = query.lower()
        
        numbers = re.findall(r'\b\d+\b', query)
        if numbers:
            target_hids = [int(n) for n in numbers]
            for f in history_files:
                if f['hid'] in target_hids:
                    selected_files.append(f)
        
        if not selected_files:
            keywords = ["matrix", "genes", "features", "barcodes", "mtx", "tsv"]
            for f in history_files:
                fname = f['name'].lower()
                if any(kw in fname for kw in keywords) and any(kw in query_lower for kw in keywords):
                     if "matrix" in query_lower and ("matrix" in fname or "mtx" in fname):
                         if f not in selected_files: selected_files.append(f)
                     if ("genes" in query_lower or "features" in query_lower) and ("genes" in fname or "features" in fname):
                         if f not in selected_files: selected_files.append(f)
                     if "barcodes" in query_lower and "barcodes" in fname:
                         if f not in selected_files: selected_files.append(f)

        return selected_files

    def smart_process(self, user_query, file_context_list, chat_history=[], selected_tool=None, tool_params=None, workflow_data=None, use_history_files=False):
        if selected_tool:
            if not tool_params:
                return self._handle_tool_selection(selected_tool, file_context_list)
            else:
                return self._generate_code_with_params(selected_tool, tool_params, file_context_list)
        
        if workflow_data:
            if not file_context_list and not use_history_files:
                history_files = self._get_history_datasets()
                if history_files:
                    return {
                        "type": "data_selector",
                        "datasets": history_files,
                        "reply": "æ£€æµ‹åˆ°æ‚¨å°šæœªé€‰æ‹©æ•°æ®æ–‡ä»¶ã€‚è¯·å…ˆå‹¾é€‰éœ€è¦ä½¿ç”¨çš„å†å²æ•°æ®ï¼Œç¡®è®¤åç³»ç»Ÿå°†è‡ªåŠ¨å…³è”ã€‚",
                        "thought": "[THOUGHT]ç”¨æˆ·ç‚¹å‡»æ‰§è¡Œä½†æœªæä¾›æ–‡ä»¶ï¼Œæ‹¦æˆªè¯·æ±‚å¹¶å¼¹å‡ºæ•°æ®é€‰æ‹©å™¨ã€‚[/THOUGHT]"
                    }
                else:
                    return {
                        "type": "text", 
                        "reply": "âŒ æœªæ£€æµ‹åˆ°å¯ç”¨æ•°æ®ã€‚è¯·å…ˆä¸Šä¼ æ–‡ä»¶æˆ–æ£€æŸ¥ Galaxy å†å²è®°å½•ã€‚", 
                        "thought": None
                    }

            run_id = wf_manager.create_task("Seurat Workflow", len(workflow_data['steps']))
            threading.Thread(target=self._execute_workflow_background, args=(run_id, workflow_data, file_context_list)).start()
            return {
                "type": "workflow_started",
                "run_id": run_id,
                "reply": "ğŸš€ å·¥ä½œæµå·²æäº¤åˆ°åå°æ‰§è¡Œï¼",
                "thought": "[THOUGHT]ä»»åŠ¡å·²ç§»äº¤åå°ä»»åŠ¡ç®¡ç†å™¨ã€‚[/THOUGHT]"
            }

        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘å…ˆè¿›è¡Œè¯­ä¹‰è·¯ç”±ï¼Œä¸å†ä¾èµ–ç¡¬ç¼–ç å…³é”®è¯
        bio_intent = self._classify_bio_intent(user_query, chat_history)

        # å¦‚æœæ˜¯ GENERALï¼Œèµ°é€šç”¨é—²èŠ
        if bio_intent == "GENERAL":
            template = """
            You are a helpful AI Assistant. User Input: {query}. History: {history}. 
            Answer directly in Chinese.
            **IMPORTANT**: First, think step-by-step inside [THOUGHT] tags.
            """
            history_text = "\n".join([f"User: {h.get('user','')}\nAI: {h.get('ai','')}" for h in chat_history[-3:]])
            prompt = ChatPromptTemplate.from_template(template)
            chain = prompt | self.brain | StrOutputParser()
            try:
                response = chain.invoke({"query": user_query, "history": history_text})
            except:
                response = "ä½ å¥½ï¼æˆ‘æ˜¯ Galaxy ç”Ÿä¿¡åŠ©æ‰‹ã€‚"
            return self._parse_llm_response(response, context="chat", user_query=user_query)

        # å¦‚æœæ˜¯ EXECUTIONï¼Œå°è¯•è‡ªåŠ¨æ‰§è¡Œ
        if bio_intent == "EXECUTION":
            # æ£€æŸ¥æ˜¯å¦æ˜¯ Seurat æµç¨‹
            if "seurat" in user_query.lower() or "æµç¨‹" in user_query or "å·¥ä½œæµ" in user_query:
                force_history = "å†å²" in user_query or "history" in user_query.lower()
                
                if not file_context_list:
                    history_files = self._get_history_datasets()
                    parsed_files = self._parse_files_from_query(user_query, history_files)
                    if parsed_files:
                        file_context_list = parsed_files 
                    elif not use_history_files and not force_history:
                        if history_files:
                            return {
                                "type": "data_selector",
                                "datasets": history_files,
                                "reply": "æ£€æµ‹åˆ°æ‚¨å½“å‰æœªä¸Šä¼ æ–‡ä»¶ï¼Œä½† Galaxy å†å²è®°å½•ä¸­å­˜åœ¨æ•°æ®ã€‚è¯·å‹¾é€‰è¦ç”¨äºæœ¬æ¬¡åˆ†æçš„æ–‡ä»¶ï¼š",
                                "thought": "[THOUGHT]ç”¨æˆ·æƒ³é€šè¿‡è‡ªç„¶è¯­è¨€æ‰§è¡Œæµç¨‹ï¼Œä½†æœªæ£€æµ‹åˆ°ä¸Šä¼ æ–‡ä»¶ï¼Œå¼•å¯¼é€‰æ‹©å†å²æ•°æ®ã€‚[/THOUGHT]"
                            }
                        else:
                            return {"type": "text", "reply": "è¯·å…ˆä¸Šä¼  Matrix/Genes/Barcodes æ–‡ä»¶ï¼Œæˆ‘æ‰èƒ½ä¸ºæ‚¨æ‰§è¡Œåˆ†æã€‚", "thought": "[THOUGHT]ç¼ºå°‘è¾“å…¥æ•°æ®ï¼Œæ— æ³•æ‰§è¡Œã€‚[/THOUGHT]"}
                
                if force_history and not file_context_list:
                     history_files = self._get_history_datasets()
                     return {
                            "type": "data_selector",
                            "datasets": history_files,
                            "reply": "å¥½çš„ï¼Œå·²ä¸ºæ‚¨åŠ è½½å†å²æ•°æ®ã€‚è¯·å‹¾é€‰æœ¬æ¬¡åˆ†æéœ€è¦ä½¿ç”¨çš„æ–‡ä»¶ï¼š",
                            "thought": "[THOUGHT]ç”¨æˆ·æ˜ç¡®è¦æ±‚ä½¿ç”¨å†å²æ•°æ®ï¼Œå¼¹å‡ºé€‰æ‹©å™¨ä¾›ç¡®è®¤ã€‚[/THOUGHT]"
                        }

                template_key = "seurat_standard"
                wf_template = WORKFLOW_TEMPLATES[template_key]
                steps_for_execution = []
                
                for step in wf_template['steps']:
                    real_tool_id = self._find_tool_id_by_keyword(step['tool_keyword'])
                    if not real_tool_id:
                        real_tool_id = "UNKNOWN_TOOL"
                    
                    steps_for_execution.append({
                        "name": step['name'],
                        "tool_keyword": step['tool_keyword'],
                        "tool_id": real_tool_id,
                        "desc": step['desc'],
                        "params": step['params'] 
                    })

                constructed_workflow_data = {
                    "workflow_name": wf_template['name'],
                    "steps": steps_for_execution
                }

                run_id = wf_manager.create_task("Seurat Workflow (Auto)", len(steps_for_execution))
                threading.Thread(target=self._execute_workflow_background, args=(run_id, constructed_workflow_data, file_context_list)).start()

                return {
                    "type": "workflow_started",
                    "run_id": run_id,
                    "reply": f"å·²ç†è§£æ‚¨çš„æŒ‡ä»¤ã€‚æ­£åœ¨åŸºäºæ‚¨æŒ‡å®šçš„ **{len(file_context_list)} ä¸ªæ–‡ä»¶**ï¼Œä½¿ç”¨æ ‡å‡†å‚æ•°æ‰§è¡Œ **{wf_template['name']}**ã€‚",
                    "thought": f"[THOUGHT]æ£€æµ‹åˆ°æ˜ç¡®çš„æ‰§è¡Œæ„å›¾å’Œæ–‡ä»¶æŒ‡å®šï¼ˆé€šè¿‡NLPè§£æï¼‰ã€‚å·²è‡ªåŠ¨åŠ è½½æ•°æ®å’Œæ¨¡æ¿ï¼Œç›´æ¥å¯åŠ¨åå°ä»»åŠ¡ã€‚[/THOUGHT]"
                }
            else:
                # å•å·¥å…·æ‰§è¡Œé€»è¾‘ (æš‚ç•¥ï¼Œå¯æ‰©å±•)
                return {"type": "text", "reply": "å•å·¥å…·æ‰§è¡Œæš‚æœªé€‚é…åå°ä»»åŠ¡æ¨¡å¼ï¼Œå»ºè®®ä½¿ç”¨å·¥ä½œæµã€‚", "thought": None}

        # å¦‚æœæ˜¯ WORKFLOW_PLANNING
        if bio_intent == "WORKFLOW_PLANNING":
            if not file_context_list:
                history_files = self._get_history_datasets()
                parsed_files = self._parse_files_from_query(user_query, history_files)
                if parsed_files:
                    file_context_list = parsed_files
                elif not use_history_files:
                    # å¦‚æœç”¨æˆ·åªæ˜¯æƒ³çœ‹æ­¥éª¤ï¼Œä¸å¼¹çª—ï¼›å¦‚æœç”¨æˆ·æƒ³è§„åˆ’å…·ä½“æ•°æ®ï¼Œå¼¹çª—
                    # è¿™é‡Œç®€åŒ–ä¸ºï¼šå¦‚æœæ²¡æœ‰æ–‡ä»¶ï¼Œä¸”ä¸æ˜¯çº¯æŸ¥çœ‹ï¼Œå°±å¼¹çª—
                    is_pure_view = "çœ‹çœ‹" in user_query or "åˆ—å‡º" in user_query
                    if not is_pure_view and history_files:
                        return {
                            "type": "data_selector",
                            "datasets": history_files,
                            "reply": "æ£€æµ‹åˆ°æ‚¨å½“å‰æœªä¸Šä¼ æ–‡ä»¶ï¼Œä½† Galaxy å†å²è®°å½•ä¸­å­˜åœ¨æ•°æ®ã€‚æ˜¯å¦ç›´æ¥ä½¿ç”¨ï¼Ÿ",
                            "thought": "[THOUGHT]ç”¨æˆ·æƒ³è·‘æµç¨‹ä½†æ²¡ä¼ æ–‡ä»¶ï¼Œæ£€æµ‹åˆ°å†å²æ•°æ®ï¼Œæ¨èå¤ç”¨ã€‚[/THOUGHT]"
                        }

            template_key = "seurat_standard"
            wf = WORKFLOW_TEMPLATES[template_key]
            steps_with_params = []
            
            for step in wf['steps']:
                real_tool_id = self._find_tool_id_by_keyword(step['tool_keyword'])
                
                frontend_params = []
                if isinstance(step['params'], dict):
                    for k, v in step['params'].items():
                        frontend_params.append({
                            "name": k,
                            "label": k,
                            "type": "text",
                            "value": v,
                            "help": "Default parameter"
                        })
                else:
                    frontend_params = step['params']

                steps_with_params.append({
                    "name": step['name'],
                    "tool_id": real_tool_id if real_tool_id else "UNKNOWN_TOOL", 
                    "desc": step['desc'],
                    "params": frontend_params 
                })
            
            reply_msg = f"å·²ä¸ºæ‚¨è§„åˆ’ **{wf['name']}**ï¼ŒåŒ…å« {len(steps_with_params)} ä¸ªæ­¥éª¤ã€‚å·²ä¸ºæ‚¨é¢„å¡«äº†**æœ€ä½³å®è·µå‚æ•°**ï¼š"
            if file_context_list:
                reply_msg += f"\n\nâœ… å·²é€‰ä¸­ {len(file_context_list)} ä¸ªæ–‡ä»¶ç”¨äºåˆ†æã€‚"

            return {
                "type": "workflow_config",
                "workflow_name": wf['name'],
                "steps": steps_with_params,
                "reply": reply_msg,
                "thought": "[THOUGHT]è¯†åˆ«åˆ°ç”¨æˆ·éœ€è¦å•ç»†èƒå…¨æµç¨‹ã€‚å·²åŠ è½½ Seurat æ ‡å‡†æ¨¡æ¿å¹¶æ³¨å…¥æ™ºèƒ½é»˜è®¤å‚æ•°ã€‚[/THOUGHT]"
            }

        # å¦‚æœæ˜¯ CONSULTATION
        if bio_intent == "CONSULTATION":
            if not self.vector_db:
                return {"type": "text", "reply": "âš ï¸ å’¨è¯¢åŠŸèƒ½éœ€è¦åŠ è½½çŸ¥è¯†åº“ (ChromaDB)ï¼Œç›®å‰æœªæ£€æµ‹åˆ°æ•°æ®ã€‚", "thought": None}
            retriever = self.vector_db.as_retriever(search_kwargs={"k": 25})
            docs = retriever.invoke(user_query)
            retrieved_tools = "\n".join([f"- {d.page_content}" for i, d in enumerate(docs)])
            template = """
            You are a Galaxy Tool Consultant. User Request: "{query}"
            ã€Available Tools (RAG)ã€‘ {retrieved_tools}
            **TASK**: List tools in JSON format.
            **MANDATORY**: 
            1. Start with [THOUGHT]...[/THOUGHT] to explain your reasoning.
            2. Then output ONLY the JSON list. NO extra text.
            Output Format: 
            [THOUGHT]...[/THOUGHT]
            ```json
            [ {{"name": "...", "id": "..."}}, ... ]
            ```
            """
            prompt = ChatPromptTemplate.from_template(template)
            chain = prompt | self.brain | StrOutputParser()
            response = chain.invoke({"query": user_query, "retrieved_tools": retrieved_tools})
            return self._parse_llm_response(response, context="chat", user_query=user_query)

        return self._parse_llm_response(response, context="chat", user_query=user_query)

    def _map_params_for_galaxy(self, tool_keyword, user_params):
        mapped = {}
        key = tool_keyword.lower()
        
        if "filtercells" in key or "filter_cells" in key:
            if "subset_name" in user_params:
                mapped["subsets_0|name"] = user_params["subset_name"]
            if "low_threshold" in user_params:
                mapped["subsets_0|low_threshold"] = user_params["low_threshold"]
            if "high_threshold" in user_params:
                mapped["subsets_0|high_threshold"] = user_params["high_threshold"]
            return mapped
            
        if "normalisedata" in key or "normalise_data" in key:
            return user_params 
            
        return user_params

    def _execute_workflow_background(self, run_id, workflow_data, file_context_list):
        steps = workflow_data['steps']
        last_output_id = None
        
        histories = gi.histories.get_histories()
        history_id = histories[0]['id']
        
        matrix_id = next((f['id'] for f in file_context_list if 'matrix' in f['name'].lower() or f['name'].endswith('.mtx')), None)
        genes_id = next((f['id'] for f in file_context_list if 'genes' in f['name'].lower() or 'features' in f['name'].lower()), None)
        barcodes_id = next((f['id'] for f in file_context_list if 'barcodes' in f['name'].lower()), None)

        print(f"   [åå°ä»»åŠ¡ {run_id}] å¼€å§‹æ‰§è¡Œ {len(steps)} ä¸ªæ­¥éª¤...")
        
        for i, step in enumerate(steps):
            tool_id = step['tool_id']
            step_name = step['name']
            user_params = step['params']
            
            if tool_id == "UNKNOWN_TOOL":
                wf_manager.fail_task(run_id, f"æ­¥éª¤ {step_name} å¯¹åº”çš„å·¥å…·åœ¨ Galaxy ä¸­æœªæ‰¾åˆ°ã€‚")
                return

            wf_manager.update_step(run_id, i, step_name, "running", f"æ­£åœ¨æäº¤ä»»åŠ¡: {step_name} (ID: {tool_id})")
            
            try:
                kw = step.get('tool_keyword', tool_id)
                tool_inputs = self._map_params_for_galaxy(kw, user_params)
                
                if i == 0 and ("create" in tool_id or "read10x" in tool_id):
                    if not (matrix_id and genes_id and barcodes_id):
                            wf_manager.fail_task(run_id, "ç¼ºå°‘å¿…è¦çš„ 10x æ–‡ä»¶ (Matrix/Genes/Barcodes)")
                            return
                    tool_inputs['method|method'] = 'CreateSeuratObject'
                    tool_inputs['method|input_type|input_type'] = 'mtx'
                    tool_inputs['method|input_type|matrix'] = {'src': 'hda', 'id': matrix_id}
                    tool_inputs['method|input_type|gene_names'] = {'src': 'hda', 'id': genes_id}
                    tool_inputs['method|input_type|cell_barcodes'] = {'src': 'hda', 'id': barcodes_id}
                else:
                    if last_output_id:
                        input_rds_id = last_output_id
                        print(f"   [Task {run_id}] Using previous step output: {input_rds_id}")
                    else:
                        rds_file = next((f for f in file_context_list if 'rds' in f['name'].lower() or 'rdata' in f['name'].lower() or 'seurat' in f['name'].lower()), None)
                        if rds_file:
                            input_rds_id = rds_file['id']
                            print(f"   [Task {run_id}] Using user-selected RDS file: {rds_file['name']} ({input_rds_id})")
                        else:
                            wf_manager.fail_task(run_id, f"é”™è¯¯ï¼šæ­¥éª¤ {step_name} éœ€è¦ RDS æ–‡ä»¶ä½œä¸ºè¾“å…¥ï¼Œä½†æœªæ‰¾åˆ°ä¸Šä¸€æ­¥è¾“å‡ºæˆ–å·²é€‰æ–‡ä»¶ä¸­çš„ RDS æ–‡ä»¶ã€‚")
                            return

                    tool_inputs['input|rds_seurat_file'] = {'src': 'hda', 'id': input_rds_id}
                
                print(f"   [Task {run_id}] Submitting tool {tool_id} with inputs: {tool_inputs}")
                ret = gi.tools.run_tool(tool_id=tool_id, tool_inputs=tool_inputs, history_id=history_id)
                
                if ret['jobs']:
                    job_id = ret['jobs'][0]['id']
                    wf_manager.update_step(run_id, i, step_name, "running", f"Job ID: {job_id} å·²æäº¤ï¼Œç­‰å¾…å®Œæˆ...")
                    
                    poll_interval = 2 
                    max_interval = 30 
                    
                    while True:
                        job_info = gi.jobs.show_job(job_id)
                        state = job_info['state']
                        
                        if state == 'ok':
                            wf_manager.update_step(run_id, i, step_name, "success", f"ä»»åŠ¡å®Œæˆã€‚")
                            if ret['outputs']:
                                last_output_id = ret['outputs'][0]['id']
                            break
                        elif state in ['error', 'deleted']:
                            stderr = job_info.get('tool_stderr', 'No stderr')
                            print(f"   [Task {run_id}] Step failed (Soft Fail). Stderr: {stderr}")
                            wf_manager.update_step(run_id, i, step_name, "failed", f"ä»»åŠ¡å¤±è´¥ (å·²è·³è¿‡): {stderr[:50]}...")
                            break 
                        
                        time.sleep(poll_interval)
                        poll_interval = min(poll_interval + 2, max_interval)
                        
                else:
                    wf_manager.fail_task(run_id, "Galaxy æœªè¿”å› Job ID")
                    return
            
            except ConnectionError as ce:
                 print(f"   [Task {run_id}] ConnectionError (400): {ce}")
                 wf_manager.update_step(run_id, i, step_name, "failed", "å‚æ•°æ ¡éªŒå¤±è´¥ (å·²è·³è¿‡)")
                 continue 
                    
            except Exception as e:
                error_msg = str(e)
                print(f"   [Task {run_id}] Exception: {traceback.format_exc()}")
                wf_manager.fail_task(run_id, f"æ‰§è¡Œå¼‚å¸¸: {error_msg}")
                return
        
        wf_manager.complete_task(run_id)

    def _handle_tool_selection(self, tool_info, file_context_list):
        tool_name = tool_info['name']
        tool_id = tool_info.get('id', '')
        if not tool_id or "unknown" in tool_id.lower():
            tool_id = self._find_tool_id_by_keyword(tool_name)
        
        if tool_id:
            params = self._fetch_galaxy_tool_params(tool_name) 
            return {"type": "tool_config", "tool_name": tool_name, "tool_id": tool_id, "params": params or [], "reply": f"å·²è·å– **{tool_name}** çš„é…ç½®å‚æ•°ï¼Œè¯·ç¡®è®¤ï¼š", "thought": f"[THOUGHT]æˆåŠŸè°ƒç”¨ Galaxy API è·å–äº†å·¥å…·å‚æ•°ç»“æ„ã€‚[/THOUGHT]"}
        else:
            return self._generate_code_only(f"Run {tool_name}", file_context_list, tool_info)

    def _generate_code_with_params(self, tool_info, user_params, file_context_list):
        files_desc = "\n".join([f"- ID: {f['id']} | Filename: {f['name']}" for f in file_context_list])
        params_json = json.dumps(user_params, indent=2, ensure_ascii=False)
        template = """
        You are a Galaxy BioBlend Expert. 
        Target Tool: "{tool_name}" (ID: {tool_id})
        ã€User Configured Parametersã€‘ {params_json}
        ã€Available Filesã€‘ {files_desc}
        **TASK**: Write Python code to run this tool.
        **CRITICAL**: 
        1. Use `gi.tools.run_tool`. 
        2. **INPUT CHAINING**: Find the latest dataset in history to use as input (if required).
        3. Map user parameters to `tool_inputs`.
        4. Start with [THOUGHT] tags.
        """
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.brain | StrOutputParser()
        response = chain.invoke({
            "tool_name": tool_info['name'],
            "tool_id": tool_info['id'],
            "params_json": params_json,
            "files_desc": files_desc
        })
        return self._parse_llm_response(response, context="execution")

    def _generate_code_only(self, query, file_context_list, tool_info):
        files_desc = "\n".join([f"- ID: {f['id']} | Filename: {f['name']}" for f in file_context_list])
        template = """
        You are a Galaxy BioBlend Expert. User selected tool "{tool_name}" (ID: {tool_id}).
        ã€Available Filesã€‘ {files_desc}
        **CRITICAL**: Use `gi`. No `if __name__`.
        **MANDATORY**: Start with [THOUGHT] tags. Output Python Code.
        """
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.brain | StrOutputParser()
        response = chain.invoke({
            "tool_name": tool_info['name'],
            "tool_id": tool_info['id'],
            "query": query,
            "files_desc": files_desc
        })
        return self._parse_llm_response(response, context="execution", user_query=query)

    def _extract_thought(self, text):
        thought = None
        clean_text = text
        patterns = [r"<think>(.*?)</think>", r"\[THOUGHT\](.*?)\[/THOUGHT\]", r"\[THOUGHT\](.*?)$"]
        for pattern in patterns:
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
            if match:
                thought = match.group(1).strip()
                clean_text = re.sub(pattern, "", text, flags=re.DOTALL | re.IGNORECASE).strip()
                break
        filler_phrases = ["Now produce the final answer.", "Here is the answer:", "Answer:", "Here is the list of tools:", "Now, I will provide the answer."]
        for phrase in filler_phrases:
            if clean_text.lstrip().startswith(phrase):
                clean_text = clean_text.replace(phrase, "", 1).strip()
        return thought, clean_text

    def _parse_llm_response(self, response, context="chat", user_query=""):
        response = response.strip()
        thought_content, clean_response = self._extract_thought(response)
        if "```python" in clean_response:
            try:
                code = clean_response.split("```python")[1].split("```")[0].strip()
                success, exec_out, error_msg = self._execute_code_sandbox(code)
                if success:
                    final_reply = f"### ğŸ¤– ç­–ç•¥ä»£ç \n```python\n{code}\n```\n### âœ… æ‰§è¡Œç»“æœ\n```text\n{exec_out}\n```"
                    return {"type": "text", "reply": final_reply, "thought": thought_content, "suggestions": ["ä¿å­˜ç»“æœ", "å¯è§†åŒ–"]}
                else:
                    diagnosis = self._analyze_error_and_guide(user_query, code, error_msg)
                    return {"type": "text", "reply": diagnosis, "thought": thought_content, "suggestions": ["é‡è¯•"]}
            except: return {"type": "text", "reply": clean_response, "thought": thought_content, "suggestions": []}
        json_candidate = None
        if "```json" in clean_response: json_candidate = clean_response.split("```json")[1].split("```")[0].strip()
        elif clean_response.strip().startswith("[") and clean_response.strip().endswith("]"): json_candidate = clean_response.strip()
        else:
            start = clean_response.find("[")
            end = clean_response.rfind("]")
            if start != -1 and end != -1 and end > start: json_candidate = clean_response[start : end + 1]
        if json_candidate:
            try:
                candidates = json.loads(json_candidate)
                return self._process_json_candidates(json.dumps(candidates), thought_content)
            except json.JSONDecodeError:
                try:
                    candidates = ast.literal_eval(json_candidate)
                    return self._process_json_candidates(json.dumps(candidates), thought_content)
                except: pass 
        return {"type": "text", "reply": clean_response, "thought": thought_content, "suggestions": []}

    def _process_json_candidates(self, json_str, thought_content):
        try:
            candidates = json.loads(json_str)
            if not isinstance(candidates, list): candidates = [candidates]
            cleaned_candidates = []
            for c in candidates:
                if isinstance(c, str):
                    if "unknown" in c.lower(): continue
                    c = {"name": c, "id": ""}
                elif isinstance(c, dict):
                    name = c.get('name', 'Unknown Tool')
                    raw_id = c.get('id', '')
                    if "unknown" in name.lower() or "unknown" in raw_id.lower(): continue
                    c['id'] = raw_id
                cleaned_candidates.append(c)
            if not cleaned_candidates: return {"type": "text", "reply": "æœªæ‰¾åˆ°ç›¸å…³å·¥å…·ã€‚", "thought": thought_content, "suggestions": []}
            return {"type": "choice", "reply": f"ä¸ºæ‚¨æ‰¾åˆ°äº† {len(cleaned_candidates)} ä¸ªç›¸å…³å·¥å…·ï¼š", "candidates": cleaned_candidates, "thought": thought_content, "suggestions": []}
        except: return {"type": "text", "reply": json_str, "thought": thought_content, "suggestions": []}

    def _analyze_error_and_guide(self, query, code, error_msg):
        chain = self.error_analysis_prompt | self.brain | StrOutputParser()
        try:
            short_error = error_msg[-2000:] if len(error_msg) > 2000 else error_msg
            diagnosis = chain.invoke({"query": query, "code_snippet": code[:500], "error_msg": short_error})
            thought, clean_diag = self._extract_thought(diagnosis)
            return clean_diag 
        except Exception as e: return f"âŒ æ‰§è¡Œå‡ºé”™ã€‚\nåŸå§‹é”™è¯¯: {str(e)}"

    def _execute_code_sandbox(self, code):
        if not gi: return False, "", "Galaxy æœªè¿æ¥"
        output_buffer = io.StringIO()
        try:
            sandbox = {"gi": gi, "json": json, "print": print}
            with contextlib.redirect_stdout(output_buffer): exec(code, sandbox)
            return True, output_buffer.getvalue() or "(æ‰§è¡ŒæˆåŠŸï¼Œæ— å±å¹•è¾“å‡º)", ""
        except Exception: return False, "", traceback.format_exc()

app = FastAPI()
templates = Jinja2Templates(directory="templates")
agent = BioBlendAgent()

class ChatRequest(BaseModel):
    message: str
    history: List[Dict[str, str]] = [] 
    selected_tool: Optional[Dict[str, Any]] = None
    tool_params: Optional[Dict[str, Any]] = None
    workflow_data: Optional[Dict[str, Any]] = None
    uploaded_files: List[Dict[str, Any]] = [] 
    use_history_files: bool = False 

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/api/upload")
async def upload_handler(file: UploadFile = File(...)):
    try:
        temp_path = f"temp_{file.filename}"
        with open(temp_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        result = {"status": "success", "file_name": file.filename}
        if file.filename.lower().endswith(('.png', '.jpg', '.jpeg')):
            ocr_text = agent.ocr_image(temp_path)
            result["type"] = "image"
            result["ocr_text"] = ocr_text
            result["suggestions"] = []
            os.remove(temp_path) 
        else:
            if gi:
                histories = gi.histories.get_histories()
                hid = histories[0]['id'] if histories else gi.histories.create_history("GPT-OSS Analysis")['id']
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        print(f"Uploading {file.filename} (Attempt {attempt+1}/{max_retries})...")
                        ret = gi.tools.upload_file(temp_path, hid)
                        result["type"] = "data"
                        result["file_id"] = ret['outputs'][0]['id']
                        result["suggestions"] = []
                        break 
                    except Exception as upload_err:
                        print(f"Upload failed: {upload_err}")
                        if attempt < max_retries - 1: time.sleep(2) 
                        else: raise upload_err
                os.remove(temp_path)
            else:
                return {"status": "error", "message": "Galaxy æœªè¿æ¥"}
        return result
    except Exception as e:
        return {"status": "error", "message": f"ä¸Šä¼ å¤±è´¥: {str(e)}"}

@app.post("/api/chat")
async def chat_handler(req: ChatRequest):
    try:
        response = agent.smart_process(
            req.message, 
            req.uploaded_files, 
            req.history, 
            req.selected_tool, 
            req.tool_params, 
            req.workflow_data,
            req.use_history_files
        )
        return response
    except Exception as e:
        traceback.print_exc()
        return JSONResponse(status_code=500, content={"type": "text", "reply": f"âŒ ç³»ç»Ÿå†…éƒ¨é”™è¯¯: {str(e)}", "thought": None, "suggestions": ["é‡è¯•"]})

@app.get("/api/workflow/status/{run_id}")
async def get_workflow_status(run_id: str):
    if run_id in wf_manager.tasks:
        return wf_manager.tasks[run_id]
    return {"status": "not_found"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=PORT)
