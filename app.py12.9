import os
import io
import json
import base64
import traceback
import contextlib
import uvicorn
import shutil
import time
import re
from fastapi import FastAPI, Request, UploadFile, File
# ã€æ ¸å¿ƒä¿®å¤ã€‘åŒæ—¶å¼•å…¥ HTMLResponse å’Œ JSONResponse
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
from typing import List, Optional, Union, Dict, Any
from dotenv import load_dotenv

# --- LangChain & BioBlend ---
from bioblend.galaxy import GalaxyInstance
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage

load_dotenv()

# ================= é…ç½®åŒºåŸŸ =================
PORT = 8082
EMBED_MODEL = "nomic-embed-text"
LLM_MODEL = "gpt-oss:latest"
VISION_MODEL = "llama3.2-vision:11b"
OLLAMA_URL = "http://localhost:11434"
VECTOR_DB_PATH = "./data/chroma_db_bioblend"
GALAXY_URL = os.getenv("GALAXY_URL", "https://usegalaxy.org")
GALAXY_KEY = os.getenv("GALAXY_API_KEY", "")

# --- å¤æ‚å·¥å…·çš„ç¡¬æ€§éœ€æ±‚é…ç½® ---
TOOL_REQUIREMENTS = {
    "seurat": {
        "required_keywords": ["matrix", "genes", "barcodes"], 
        "min_files": 3,
        "guide_msg": """
        âš ï¸ **Seurat Read10x éœ€è¦ 3 ä¸ªæ ‡å‡†æ–‡ä»¶** (10x Genomics æ ¼å¼)ï¼š
        1. **Expression Matrix** (æ–‡ä»¶åéœ€åŒ…å« `matrix`)
        2. **Gene Table** (æ–‡ä»¶åéœ€åŒ…å« `genes` æˆ– `features`)
        3. **Barcode Table** (æ–‡ä»¶åéœ€åŒ…å« `barcodes`)
        è¯·ç»§ç»­ä¸Šä¼ ç¼ºå¤±çš„æ–‡ä»¶ã€‚é›†é½åå†æ¬¡ç‚¹å‡»è¿è¡Œï¼Œæˆ‘å°†è‡ªåŠ¨ä¸ºæ‚¨æ˜ å°„å‚æ•°ã€‚
        """
    }
}
# ===========================================

gi = None
print(f">>> [ç³»ç»Ÿ] æ­£åœ¨è¿æ¥ Galaxy ({GALAXY_URL})...")
try:
    gi = GalaxyInstance(url=GALAXY_URL, key=GALAXY_KEY)
    user = gi.users.get_current_user()
    print(f">>> [ç³»ç»Ÿ] Galaxy è¿æ¥æˆåŠŸ! å½“å‰ç”¨æˆ·: {user.get('username', 'Unknown')}")
except Exception as e:
    print(f">>> [ä¸¥é‡é”™è¯¯] Galaxy è¿æ¥å¤±è´¥: {e}")

class BioBlendAgent:
    def __init__(self):
        self.vector_db = self._load_db()
        self.brain = ChatOllama(model=LLM_MODEL, base_url=OLLAMA_URL, temperature=0.1)
        self.eye = ChatOllama(model=VISION_MODEL, base_url=OLLAMA_URL, temperature=0)
        
        self.system_capabilities = """
        [Source A: System Kernel (Management)]
        1. Get Current User Info: gi.users.get_current_user()
        2. List Histories: gi.histories.get_histories()
        3. Upload File: gi.tools.upload_file('path', history_id)
        """
        
        self.error_analysis_prompt = ChatPromptTemplate.from_template("""
        You are a Galaxy Bioinformatics Error Analyst. 
        
        ã€Contextã€‘
        - User Intent: "{query}"
        - Tool/Code Attempted: "{code_snippet}"
        - Error Message: "{error_msg}"
        
        ã€Taskã€‘
        1. Analyze the error message.
        2. Provide a friendly, corrective guide in **Chinese**.
        
        **IMPORTANT**: First, explain your reasoning inside <think> tags. Then provide the final answer.
        """)

    def _load_db(self):
        if os.path.exists(VECTOR_DB_PATH) and os.listdir(VECTOR_DB_PATH):
            embeddings = OllamaEmbeddings(model=EMBED_MODEL, base_url=OLLAMA_URL)
            return Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=embeddings)
        return None

    def ocr_image(self, image_path):
        try:
            with open(image_path, "rb") as image_file:
                image_data = base64.b64encode(image_file.read()).decode("utf-8")
            message = HumanMessage(content=[
                {"type": "text", "text": "Extract text and describe this image in Chinese."},
                {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{image_data}"}}
            ])
            res = self.eye.invoke([message])
            return res.content
        except Exception as e:
            return f"OCR è¯†åˆ«å¤±è´¥: {str(e)}"

    def _analyze_domain(self, query):
        template = """
        You are a classifier. Analyze the user's input and determine the **Domain**.
        User Input: "{query}"
        Rules for [GALAXY_BIO]: Keywords: Galaxy, BioBlend, tools, fastq, bam, genome, sequencing, workflow, analysis, QC, Seurat.
        Rules for [GENERAL]: General Python errors, Chit-chat.
        Output ONLY one word: "GALAXY_BIO" or "GENERAL".
        """
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.brain | StrOutputParser()
        try:
            return chain.invoke({"query": query[:1000]}).strip()
        except:
            return "GALAXY_BIO"

    # å‰ç½®æ„å›¾åˆ†ç±»å™¨
    def _classify_bio_intent(self, query):
        template = """
        Classify the user's intent into one of two categories:
        
        1. **CONSULTATION**: User wants to find, list, or recommend tools. (e.g., "List Seurat tools", "How to do QC?")
        2. **EXECUTION**: User wants to check system status, account info, or run a specific task. (e.g., "Check my account", "Run PCA", "Show history")
        
        User Input: "{query}"
        
        Output ONLY one word: "CONSULTATION" or "EXECUTION".
        """
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.brain | StrOutputParser()
        try:
            intent = chain.invoke({"query": query}).strip().upper()
            if "CONSULTATION" in intent: return "CONSULTATION"
            return "EXECUTION"
        except:
            return "EXECUTION" 

    def _identify_tool_key(self, tool_name):
        name = tool_name.lower()
        if "seurat" in name or "read10x" in name: return "seurat"
        return "unknown"

    def smart_process(self, user_query, file_context_list, chat_history=[], selected_tool=None):
        if not self.vector_db:
            return {"type": "text", "reply": "âŒ é”™è¯¯ï¼šçŸ¥è¯†åº“æœªåŠ è½½ã€‚", "suggestions": []}

        # 0. å¼ºåˆ¶æ‰§è¡Œé€šé“
        if selected_tool:
            tool_key = self._identify_tool_key(selected_tool['name'])
            if tool_key in TOOL_REQUIREMENTS:
                req = TOOL_REQUIREMENTS[tool_key]
                uploaded_names = [f['name'].lower() for f in file_context_list]
                missing = [kw for kw in req['required_keywords'] if not any(kw in name for name in uploaded_names)]
                if missing:
                    return {
                        "type": "text",
                        "reply": req['guide_msg'] + f"\n\n**å½“å‰ç¼ºå¤±**: {', '.join(missing)}\n**å·²ä¸Šä¼ **: {[f['name'] for f in file_context_list]}",
                        "suggestions": ["å¦‚ä½•è·å–ç¤ºä¾‹æ•°æ®ï¼Ÿ", "ä¸Šä¼  matrix.mtx", "ä¸Šä¼  genes.tsv"]
                    }
            if not file_context_list:
                 return {"type": "text", "reply": f"æ‚¨é€‰æ‹©äº†è¿è¡Œ **{selected_tool['name']}**ï¼Œä½†è¿™éœ€è¦è¾“å…¥æ•°æ®ã€‚\nè¯·ç‚¹å‡»ä¸‹æ–¹çš„ ğŸ“‚ æŒ‰é’®ä¸Šä¼ æ–‡ä»¶ã€‚", "suggestions": ["å¦‚ä½•è·å–ç¤ºä¾‹æ•°æ®ï¼Ÿ"]}
            return self._generate_code_only(user_query, file_context_list, selected_tool)

        # 1. é¢†åŸŸæ„ŸçŸ¥
        domain = self._analyze_domain(user_query)

        # åˆ†æ”¯ A: é€šç”¨/é—²èŠ
        if "GENERAL" in domain:
            template = """
            You are a helpful AI Assistant. 
            User Input: {query}. History: {history}. 
            Answer directly in Chinese.
            **IMPORTANT**: First, think step-by-step inside <think> tags. Then provide the response.
            """
            history_text = "\n".join([f"User: {h.get('user','')}\nAI: {h.get('ai','')}" for h in chat_history[-3:]])
            prompt = ChatPromptTemplate.from_template(template)
            chain = prompt | self.brain | StrOutputParser()
            response = chain.invoke({"query": user_query, "history": history_text})
            return self._parse_llm_response(response, context="chat", user_query=user_query)

        # åˆ†æ”¯ B: ç”Ÿä¿¡/Galaxy ä¸“å®¶
        else:
            # å‰ç½®æ„å›¾åˆ†æµ
            bio_intent = self._classify_bio_intent(user_query)
            history_text = "\n".join([f"User: {h.get('user','')}\nAI: {h.get('ai','')}" for h in chat_history[-3:]])

            if bio_intent == "CONSULTATION":
                # === åˆ†æµ 1: å’¨è¯¢æ¨¡å¼ (æŸ¥å·¥å…·) ===
                retriever = self.vector_db.as_retriever(search_kwargs={"k": 25})
                docs = retriever.invoke(user_query)
                self.current_docs = docs 
                retrieved_tools = "\n".join([f"- {d.page_content}" for i, d in enumerate(docs)])

                template = """
                You are a Galaxy Tool Consultant.
                User Request: "{query}"
                
                ã€Available Tools (RAG)ã€‘
                {retrieved_tools}
                
                **TASK**: List the relevant tools in JSON format.
                
                **MANDATORY**: 
                1. Start with <think> tags to analyze the request.
                2. Output JSON: ```json [ {{"name": "...", "id": "..."}}, ... ] ```
                """
                
                prompt = ChatPromptTemplate.from_template(template)
                chain = prompt | self.brain | StrOutputParser()
                response = chain.invoke({
                    "query": user_query,
                    "retrieved_tools": retrieved_tools
                })
                
            else:
                # === åˆ†æµ 2: æ‰§è¡Œ/ç®¡ç†æ¨¡å¼ (æŸ¥è´¦æˆ·ã€è·‘ä»»åŠ¡) ===
                template = """
                You are a Galaxy System Executor.
                User Request: "{query}"
                
                ã€System Capabilitiesã€‘
                {system_caps}
                
                ã€Contextã€‘
                - Uploaded Files: {file_context_list}
                - History: {history}
                
                **TASK**: Write Python code using `bioblend` to fulfill the request.
                - If checking account: `print(gi.users.get_current_user())`
                - If running tool: `gi.tools.run_tool(...)`
                
                **MANDATORY**: 
                1. Start with <think> tags to plan the code.
                2. Output Python Code: ```python ... ```
                """
                
                prompt = ChatPromptTemplate.from_template(template)
                chain = prompt | self.brain | StrOutputParser()
                response = chain.invoke({
                    "query": user_query,
                    "system_caps": self.system_capabilities,
                    "file_context_list": str(file_context_list),
                    "history": history_text
                })

            return self._parse_llm_response(response, context="chat", user_query=user_query)

    def _generate_code_only(self, query, file_context_list, tool_info):
        files_desc = "\n".join([f"- ID: {f['id']} | Filename: {f['name']}" for f in file_context_list])
        
        template = """
        You are a Galaxy BioBlend Expert. User selected tool "{tool_name}" (ID: {tool_id}).
        ã€Available Filesã€‘ {files_desc}
        ã€User Requestã€‘ "{query}"
        
        **CRITICAL RULES**:
        1. **History ID**: NEVER use placeholders. Use `gi.histories.get_histories()[0]['id']`.
        2. **File IDs**: Use the exact IDs provided.
        3. **Output**: Start with <think>...</think>, then return Python code inside ```python```.
        """
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | self.brain | StrOutputParser()
        response = chain.invoke({
            "tool_name": tool_info['name'],
            "tool_id": tool_info['id'],
            "query": query,
            "files_desc": files_desc
        })
        return self._parse_llm_response(response, context="execution", user_query=query)

    def _extract_thought(self, text):
        pattern = r"<think>(.*?)</think>"
        match = re.search(pattern, text, re.DOTALL)
        if match:
            thought = match.group(1).strip()
            clean_text = re.sub(pattern, "", text, flags=re.DOTALL).strip()
            return thought, clean_text
        else:
            return None, text

    def _parse_llm_response(self, response, context="chat", user_query=""):
        response = response.strip()
        thought_content, clean_response = self._extract_thought(response)
        
        # 1. Python ä»£ç  (æ‰§è¡Œ/ç®¡ç†)
        if "```python" in clean_response:
            try:
                code = clean_response.split("```python")[1].split("```")[0].strip()
                success, exec_out, error_msg = self._execute_code_sandbox(code)
                
                if success:
                    final_reply = f"### ğŸ¤– ç­–ç•¥ä»£ç \n```python\n{code}\n```\n### âœ… æ‰§è¡Œç»“æœ\n```text\n{exec_out}\n```"
                    return {"type": "text", "reply": final_reply, "thought": thought_content, "suggestions": ["ä¿å­˜ç»“æœ", "å¯è§†åŒ–"]}
                else:
                    print(f"   [é”™è¯¯] ä»£ç æ‰§è¡Œå¤±è´¥ï¼Œå¯åŠ¨è¯Šæ–­... \n{error_msg}")
                    diagnosis = self._analyze_error_and_guide(user_query, code, error_msg)
                    return {"type": "text", "reply": diagnosis, "thought": thought_content, "suggestions": ["å¦‚ä½•åˆ›å»º Seurat å¯¹è±¡ï¼Ÿ", "é‡æ–°ä¸Šä¼ æ–‡ä»¶"]}
            except:
                 return {"type": "text", "reply": clean_response, "thought": thought_content, "suggestions": []}
        
        # 2. JSON åˆ—è¡¨ (å’¨è¯¢)
        elif "```json" in clean_response:
            return self._process_json_candidates(clean_response.split("```json")[1].split("```")[0].strip(), thought_content)
        elif "[" in clean_response and "]" in clean_response:
            try:
                start = clean_response.find("[")
                end = clean_response.rfind("]") + 1
                potential_json = clean_response[start:end]
                json.loads(potential_json) 
                return self._process_json_candidates(potential_json, thought_content)
            except:
                return {"type": "text", "reply": clean_response, "thought": thought_content, "suggestions": []}
        
        # 3. æ™®é€šæ–‡æœ¬
        else:
            return {"type": "text", "reply": clean_response, "thought": thought_content, "suggestions": []}

    def _process_json_candidates(self, json_str, thought_content):
        try:
            candidates = json.loads(json_str)
            if not isinstance(candidates, list): candidates = [candidates]
            
            cleaned_candidates = []
            for c in candidates:
                if isinstance(c, str):
                    if "unknown" in c.lower(): continue
                    cleaned_candidates.append({"name": c, "id": ""})
                elif isinstance(c, dict):
                    name = c.get('name', 'Unknown Tool')
                    raw_id = c.get('id', '')
                    if "unknown" in name.lower() or "unknown" in raw_id.lower(): continue
                    c['id'] = raw_id
                    cleaned_candidates.append(c)

            if not cleaned_candidates:
                return {
                    "type": "text", 
                    "reply": "æœªèƒ½æ‰¾åˆ°ç›¸å…³çš„ Galaxy å·¥å…·ï¼Œè¯·å°è¯•æ›´å…·ä½“çš„æè¿°ã€‚", 
                    "thought": thought_content, 
                    "suggestions": []
                }

            return {
                "type": "choice", 
                "reply": f"ä¸ºæ‚¨æ‰¾åˆ°äº† {len(cleaned_candidates)} ä¸ªç›¸å…³å·¥å…·ï¼š", 
                "candidates": cleaned_candidates, 
                "thought": thought_content,
                "suggestions": []
            }
        except:
            return {"type": "text", "reply": json_str, "thought": thought_content, "suggestions": []}

    def _analyze_error_and_guide(self, query, code, error_msg):
        chain = self.error_analysis_prompt | self.brain | StrOutputParser()
        try:
            short_error = error_msg[-2000:] if len(error_msg) > 2000 else error_msg
            diagnosis = chain.invoke({
                "query": query,
                "code_snippet": code[:500],
                "error_msg": short_error
            })
            thought, clean_diag = self._extract_thought(diagnosis)
            return clean_diag 
        except Exception as e:
            return f"âŒ æ‰§è¡Œå‡ºé”™ï¼Œä¸”è¯Šæ–­æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚\nåŸå§‹é”™è¯¯: {str(e)}"

    def _execute_code_sandbox(self, code):
        if not gi: return False, "", "Galaxy æœªè¿æ¥"
        output_buffer = io.StringIO()
        try:
            sandbox = {"gi": gi, "json": json, "print": print}
            with contextlib.redirect_stdout(output_buffer):
                exec(code, sandbox)
            return True, output_buffer.getvalue() or "(æ‰§è¡ŒæˆåŠŸï¼Œæ— å±å¹•è¾“å‡º)", ""
        except Exception:
            err = traceback.format_exc()
            return False, "", err

app = FastAPI()
templates = Jinja2Templates(directory="templates")
agent = BioBlendAgent()

class ChatRequest(BaseModel):
    message: str
    history: List[Dict[str, str]] = [] 
    selected_tool: Optional[Dict[str, Any]] = None
    uploaded_files: List[Dict[str, str]] = [] 

@app.get("/", response_class=HTMLResponse)
async def read_root(request: Request):
    return templates.TemplateResponse("index.html", {"request": request})

@app.post("/api/upload")
async def upload_handler(file: UploadFile = File(...)):
    try:
        temp_path = f"temp_{file.filename}"
        with open(temp_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        result = {"status": "success", "file_name": file.filename}
        
        if file.filename.lower().endswith(('.png', '.jpg', '.jpeg')):
            ocr_text = agent.ocr_image(temp_path)
            result["type"] = "image"
            result["ocr_text"] = ocr_text
            result["suggestions"] = []
            os.remove(temp_path) 
        else:
            if gi:
                histories = gi.histories.get_histories()
                hid = histories[0]['id'] if histories else gi.histories.create_history("GPT-OSS Analysis")['id']
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        print(f"Uploading {file.filename} (Attempt {attempt+1}/{max_retries})...")
                        ret = gi.tools.upload_file(temp_path, hid)
                        result["type"] = "data"
                        result["file_id"] = ret['outputs'][0]['id']
                        result["suggestions"] = []
                        break 
                    except Exception as upload_err:
                        print(f"Upload failed: {upload_err}")
                        if attempt < max_retries - 1: time.sleep(2) 
                        else: raise upload_err
                os.remove(temp_path)
            else:
                return {"status": "error", "message": "Galaxy æœªè¿æ¥"}
        return result
    except Exception as e:
        return {"status": "error", "message": f"ä¸Šä¼ å¤±è´¥: {str(e)}"}

@app.post("/api/chat")
async def chat_handler(req: ChatRequest):
    try:
        response = agent.smart_process(req.message, req.uploaded_files, req.history, req.selected_tool)
        return response
    except Exception as e:
        traceback.print_exc()
        return JSONResponse(
            status_code=500, 
            content={
                "type": "text", 
                "reply": f"âŒ ç³»ç»Ÿå†…éƒ¨é”™è¯¯: {str(e)}", 
                "thought": None,
                "suggestions": ["é‡è¯•"]
            }
        )

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=PORT)
